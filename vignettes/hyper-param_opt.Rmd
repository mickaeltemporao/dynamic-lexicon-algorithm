---
title: "Hyper-parameters Optimisation"
author: "FREDON Louis"
date: "28/06/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Tunnig des hyperparametres de l'algorithme


Dans ce papier nous allons chercher à trouver les hyperparamètres à choisir pour améliorer la précision de notre algorithme.

Les hyperparamètres que nous voulons fixer sont la taille du sample de la fonction algo_10 ainsi que le nombre de sample à effectuer.

Pour réaliser cela nous allons fixer le nombre de sample (à 10), définir la meilleure taille puis trouver le nombre idéale par la suite en fixante la taille choisi.


```{r,eval=FALSE}

taille_vector<-c(0.03*dim(dfm_fixture)[1],0.05*dim(dfm_fixture)[1],0.1*dim(dfm_fixture)[1],0.2*dim(dfm_fixture)[1],15,20)

#nombre_sample<-c(2,4,6,8,10,12,14)

z<-10#on fixe le nombre de sample a 10
data_opt<-data.frame(taille_vector)
v<-0#pour le tableau avec les means de score



#for(z in nombre_sample){
for (j in taille_vector){
  
start_time = Sys.time()



a <- seq(1,z)
data_cor <- data.frame(a)
colnames(data_cor)[1] <- "numéro du sample"
remove(a)



for (i in 1:z){

  X1 <- sample(nrow(dfm_fixture), size=ceiling(j))

  for(w in 1:length(X1)){
    data_cor[i,w+1]<-X1[w]
    colnames(data_cor)[w+1]<-paste0("donnée de calib",w)
  }


  x <- calibrate(dfm_fixture,complet=T,X1)

  data_users <- x[[2]]
  word_df <- x[[1]]
  opini_target <- x[[3]]

  #use weight on the other

  y <- use_weight(data_users,rownames(word_df),word_df)
  opini_users <- y[[1]]
  word_wei <- y[[2]]

  ## Data frame des opinions de tous

  library(dplyr)
  require(rpart)
  opinions_df <- rbind(opini_target, opini_users)
  opinions_df$users <- as.numeric(opinions_df$users)
  opinions_df$opinions <- as.numeric(opinions_df$opinions)


  ###Validation

  opinions_df_arrange <- arrange(opinions_df,users)

  df_validation_arrange <- arrange(df_validation,"users_id")

  op_match <- merge(opinions_df_arrange,df_validation_arrange,by.x = "users",by.y = "users_id")


  validation_metrics <- cor(op_match$opinions,op_match[,3])


  data_cor[i,length(X1)+2] <- validation_metrics
  colnames(data_cor)[length(X1)+2] <- "validation_score"

  #data_cor[i,length(X1)+3] <- x2y(op_match$opinions,op_match[,3])[2]
  #colnames(data_cor)[length(X1)+3] <- "validation_score_x2y"
  
  #file_data<-paste0("~/Code Projet/vigilant-octo-invention/data_cor_",z,"sample_de_",j,".RData")
  #save(data_cor, file = file_data )

}
v<-v+1
end_time = Sys.time()
mean_score<-mean(abs(data_cor[,length(X1)+2]))
data_opt[v,2]<-mean_score
colnames(data_opt)[2]<-"moyenne de score"
sd_score<-sd(abs(data_cor[,length(X1)+2]))
data_opt[v,3]<-sd_score
colnames(data_opt)[3]<-"standard deviation score"
data_opt[v,4]<-end_time - start_time
colnames(data_opt)[4]<-"time of running"

}
#}

```


Les données sont enregistrées dans le data " data_opt_10sample":

```{r}
data_opt_10sample
```

Faisons une courbe de performance pour analyser les résultats :

```{r}
library(ggplot2)
library(gridExtra)

co<-ggplot(data_opt_10sample, aes(x = taille_vector,y = `moyenne de score`)) + geom_point() + geom_line(color="red")
xy<-ggplot(data_opt_10sample, aes(x = taille_vector,y = `standard deviation score`)) + geom_point() + geom_line(color="blue")

grid.arrange(co,xy, ncol=1, nrow = 2)

```


On remarque un maximum a la taille 15 cependant, la meilleure taille de vecteur à choisir est celle de 20% de la taille du dfm ( 10 ici pour nous ), car c'est celle avec la meilleure moyenne de score mais la variation la plus petite. 
En effet la taille 15 donne un meilleure score mais une variation et une durée d'algorithme plus grande. 

On garde donc la taille du vecteur de 10 soit 20% de la taille du dfm. 
On fixe donc la taille a 20%.

Effectuons les mêmes tests sur le nombre maintenant:



```{r,eval=FALSE}

#taille_vector<-c(0.03*dim(dfm_fixture)[1],0.05*dim(dfm_fixture)[1],0.1*dim(dfm_fixture)[1],0.2*dim(dfm_fixture)[1],15,20)

nombre_sample<-c(2,4,6,8,10,12,14)

j<-0.2*dim(dfm_fixture)[1] #on fixe la taille a 20%

data_opt<-data.frame(nombre_sample)
v<-0#pour le tableau avec les means de score



for(z in nombre_sample){
#for (j in taille_vector){
  
start_time = Sys.time()



a <- seq(1,z)
data_cor <- data.frame(a)
colnames(data_cor)[1] <- "numéro du sample"
remove(a)



for (i in 1:z){

  X1 <- sample(nrow(dfm_fixture), size=ceiling(j))

  for(w in 1:length(X1)){
    data_cor[i,w+1]<-X1[w]
    colnames(data_cor)[w+1]<-paste0("donnée de calib",w)
  }


  x <- calibrate(dfm_fixture,complet=T,X1)

  data_users <- x[[2]]
  word_df <- x[[1]]
  opini_target <- x[[3]]

  #use weight on the other

  y <- use_weight(data_users,rownames(word_df),word_df)
  opini_users <- y[[1]]
  word_wei <- y[[2]]

  ## Data frame des opinions de tous

  library(dplyr)
  require(rpart)
  opinions_df <- rbind(opini_target, opini_users)
  opinions_df$users <- as.numeric(opinions_df$users)
  opinions_df$opinions <- as.numeric(opinions_df$opinions)


  ###Validation

  opinions_df_arrange <- arrange(opinions_df,users)

  df_validation_arrange <- arrange(df_validation,"users_id")

  op_match <- merge(opinions_df_arrange,df_validation_arrange,by.x = "users",by.y = "users_id")


  validation_metrics <- cor(op_match$opinions,op_match[,3])


  data_cor[i,length(X1)+2] <- validation_metrics
  colnames(data_cor)[length(X1)+2] <- "validation_score"

  #data_cor[i,length(X1)+3] <- x2y(op_match$opinions,op_match[,3])[2]
  #colnames(data_cor)[length(X1)+3] <- "validation_score_x2y"
  
  #file_data<-paste0("~/Code Projet/vigilant-octo-invention/data_cor_",z,"sample_de_",j,".RData")
  #save(data_cor, file = file_data )

}
v<-v+1
end_time = Sys.time()
mean_score<-mean(abs(data_cor[,length(X1)+2]))
data_opt[v,2]<-mean_score
colnames(data_opt)[2]<-"moyenne de score"
sd_score<-sd(abs(data_cor[,length(X1)+2]))
data_opt[v,3]<-sd_score
colnames(data_opt)[3]<-"standard deviation score"
data_opt[v,4]<-end_time - start_time
colnames(data_opt)[4]<-"time of running"

}
#}


```



Les résultats sont enregistrées dans le data frame "data_opt_20calib_vector:

```{r}
data_opt_20calib_vector
```


Faisons une courbe de performance pour analyser les résultats :

```{r}
library(ggplot2)

ggplot(data_opt_20calib_vector, aes(x = nombre_sample,y = `moyenne de score`)) + geom_point() + geom_line()


```

On remarque que la meilleure moyenne de score est atteinte lorsque 4 sample sont faits, de plus la variation des scores est très petites et la durée également.
On garde donc le nombre de sample à 4.
