---
title: "Dynamical-Lexicon-Approach how to use the package DLA"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{comment-utiliser-mon-package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(DLA)
```

### Table des matières
* [Data Fixure](# Data Fixure)
    * [Random data fixure](#Random data fixure)
    * [Data fixure weighted](#Data fixure weighted)
* [DFM genration process](# DFM genration process)
* [Estimation des poids et prévision de la target](#Estimation des poids et prévision de la target)
    * [Calibration/Estimation des paramètres](#Calibration/Estimation des paramètres)
    * [Prédiction](#Prédiction)
* [Validation](#Validation)
* [Optimisation](#Optimisation)
* [Annexes](#Annexes)


### __**Data Fixure<a class="anchor" id="Data Fixure"></a>**__


Pour la vérification de nos résultats nous allons créer deux fixure, l'une avec des valeurs random et l'autre avec des valeurs calculées en fonction de l'utilisation de certains mots que l'on va choisir.

#### __**Random data fixure<a class="anchor" id="Random data fixure"></a>**__

Nous allons créer une fonction qui va générer un DFM ainsi qu'une validation aléatoire :

Nous prenons les mots des discours des présidents américains depuis 2011, soit trois présidents (disponible dans le package quanteda) :

```{r}
make_random_data_fixure<-function(nb_users){


library(quanteda)
library(magrittr)
  ########## Creation du DFM


  #On crée un data avec les politiciens, ici on prends des mots des présidents américains lors de leurs discours

data_pol<-data_corpus_inaugural %>% corpus_subset(Year > 2011) %>%tokens()
```


Maintenant nous transformons en Documents-Features-Matrix:

```{r}
  data_pol<-dfm(data_pol)# on transforme en dfm
```

On applique maintenant les filtres que nous avons choisis pour la fonction de la génération du DFM, sur la longueur du mot :

```{r}
 data_pol<-dfm_keep(data_pol, min_nchar = 2)#on garde que les mots de plus de 2 lettres
```

On enlève les "stopwords" donc les mots de ponctuation etc..

```{r}
data_pol<-dfm_remove(data_pol, pattern = stopwords("en"))#on enleve les mots communs (ponctuation, et ..)
```

On le transforme en data frame pour les manipulations :

```{r}
  data_pol<-as.data.frame(data_pol)
```

On transforme la première colonne en nom des lignes pour avoir en nom de ligne les ids et en nom de colonnes les documents :

```{r}
  rownames(data_pol)<-data_pol[,1]
  data_pol<-data_pol[,-1]


  dim(data_pol) # 1343 features
```


Maintenant que nous avons crée un data avec les politiciens, nous allons créer aléatoirement (suivant une loi normale) des users et des occurences des mots utilisés par ces utilisateurs :

```{r}
  # On crée un data users avec des occurences aléatoires

  a<-c((1:nb_users)) # colonnes des users


  b<-abs(c(rnorm(nb_users, mean=0.4, sd=3))) # occurences
  b<-round(b)

  data_users<-data.frame(a,b)
  data_users<-data_users[,-1]
  data_users<-as.data.frame(data_users)

  for(i in 2:dim(data_pol)[2]){
    data_users[,i]<-round(abs(c(rnorm(nb_users, mean=0.4, sd=3)))) # on rajoute autant qu'il y a de mots
  }



```


Maintenant nous pouvons fusionner les deux datas ( users et politiciens )

```{r}
  colnames(data_users)<-colnames(data_pol) # on mets les mêmes noms pour fusionner

  data<-rbind(data_pol, data_users)

```

Nous allons rajouter une colonne permettant de différencier les politicens des uitilisateurs lambdas, nous allons appeler cette colonne ID, si ID = 1 alors c'est un politiciens.
```{r}

  data[,dim(data)[2]+1]<-1



  for(i in 1:dim(data)[1]){
    if(i<4){
      data[i,dim(data)[2]]=1
    }
    else{data[i,dim(data)[2]]=0}
  }

  data[,dim(data)[2]]

  colnames(data)[dim(data)[2]] <- "ID"


  class(colnames(data))




```

Nous avons donc maintenant notre data fixure finale avec des occurrences aléatoires.

```{r}
  dfm_fixure<-data# data finale
```


Créons mainteant le data  fixure de prédiction, toujours aléatoirement et suivant une loi normale :

```{r}


  ########### Creation du Data frame validation

  df_validation<-data.frame(rownames(data_users))
  colnames(df_validation)<-'users_id'
  for(i in 1:dim(df_validation)[1]){
    df_validation[i,2]<-rnorm(1, mean=0.4, sd=3)
  }



 
```

On retourne ces deux data frame à la sortie de la fonction :

```{r}
 return(list(dfm_fixure=dfm_fixure,df_validation=df_validation))

}


```



#### __**Data fixure weighted<a class="anchor" id="Data fixure weighted"></a>**__

Nous allons maintenant créer une fixure qui va un peu plus correspondre aux résultats que nous aurons avec les fonctions du package. Pour cela on va se fixer quatre mots et quatres poids pour ces mots et les appliquées au target.

On va créer le dfm comme précedemment puis appliquer le poids selon son utilisation pour prédire le score :

```{r}
data_fixure<-function(){


  library(quanteda)
  library(magrittr)
  ########## Creation du DFM


  #On crée un data avec les politiciens, ici on prends des mots des présidents américains lors de leurs discours

  data_pol<-data_corpus_inaugural %>% corpus_subset(Year > 2011) %>%tokens()
  data_pol<-dfm(data_pol)# on transforme en dfm
  data_pol<-dfm_keep(data_pol, min_nchar = 2)#on garde que les mots de plus de 2 lettres
  data_pol<-dfm_remove(data_pol, pattern = stopwords("en"))#on enleve les mots communs (ponctuation, et ..)
  data_pol<-as.data.frame(data_pol)
  rownames(data_pol)<-data_pol[,1]
  data_pol<-data_pol[,-1]


  dim(data_pol) # 1343 features

  # On crée un data users avec des occurences aléatoires

  a<-c((1:100)) # colonnes des users


  b<-abs(c(rnorm(100, mean=0.4, sd=3))) # occurences
  b<-round(b)

  data_users<-data.frame(a,b)
  data_users<-data_users[,-1]
  data_users<-as.data.frame(data_users)

  for(i in 2:dim(data_pol)[2]){
    data_users[,i]<-round(abs(c(rnorm(100, mean=0.4, sd=3)))) # on rajoute autant qu'il y a de mots
  }

  colnames(data_users)<-colnames(data_pol) # on mets les mêmes noms pour fusionner

  data<-rbind(data_pol, data_users)

  data[,dim(data)[2]+1]<-1



  for(i in 1:dim(data)[1]){
    if(i<4){
      data[i,dim(data)[2]]=1
    }
    else{data[i,dim(data)[2]]=0}
  }

  data[,dim(data)[2]]

  colnames(data)[dim(data)[2]] <- "ID"


  class(colnames(data))

  data<-data[4:103,1:1343]

  dfm_fixure<-data# data finale

```

Jusque là nous avons créée de la même manière que le random fixure, maintenant nous allons choisir 4 mots qui seront technologies, democracy, patriotism et terrorism. On décide d'appliquer un poids de -1.5 au mot technologies, un poids de -3 pour le mots democracy, un poids de 3 pour le mots patriotism et 1.5 pour le mots terrorism. Et nous calculons le score selon l'occurrence de ces mots des utilisateurs :


```{r}

  ########### Creation du Data frame validation

  df_validation<-data.frame(rownames(dfm_fixure))
  colnames(df_validation)<-"users_id"
  for(i in 1:dim(df_validation)[1]){
    df_validation[i,2]<-rnorm(1, mean=0.4, sd=3)
  }


  for (j in 1:dim(dfm_fixure)[1]){
     df_validation[j,2]=dfm_fixure[j,"technologies"]*(-1.5)+df_validation[j,2]
     df_validation[j,2]=dfm_fixure[j,"democracy"]*(-3)+df_validation[j,2]
     df_validation[j,2]=dfm_fixure[j,"patriotism"]*3+df_validation[j,2]
     df_validation[j,2]=dfm_fixure[j,"terrorism"]*1.5+df_validation[j,2]
  }




  return(list(dfm_fixure=dfm_fixure,df_validation=df_validation))

}

```






### __**DFM genration process<a class="anchor" id="DFM genration process"></a>**__


Nous allons maintenant créer une fonction qui va créer le Documents-Features-Matrix nous permettant de trouver un sous-espace de mots idéale pour la prédiction. 

Pour cela on suppose que l'utilisateur donne en entrée un data frame avec au minimum une colonne ayant le texte et une colonne ayant les documents, ainsi que le numéro de la colonne où est le texte et le numéro de colonne où est les ids. L'utilisateur devra également rentrer la langue des tweets au format international (ex : fr, en ...)

```{r}
dfm_generation<-function(
  input, #2 col (text & users )
  text,  #numéro de colonne ou il y a le text
  doc,   #numéro de colonne ou il y a les ids
  lg  # "en", "fr" etc..
  ){
```

On va devoir créer un objet token pour decompter les mots dans le texte, on utilise la fonction du package quanteda :

```{r}
library(quanteda)
tok<-tokens(input[,text], remove_punct = TRUE,remove_symbols=TRUE,remove_numbers=TRUE,remove_url=TRUE,remove_separators=TRUE)
```


Maintenant que nous avons l'objet tokens on va pouvoir créer un DFM toujours à l'aide la fonction du même package :

```{r}
df<-dfm(tok)
```

Pour définir un sous espace de mots qui est vraiment représentatif nous décidons de poser quelques filtres, notamment la longueur du mot qui doit être d'au moins 3 lettres :

```{r}
dfstop<-dfm_keep(dfstop, min_nchar = 3)
```

On enleve les mots inutiles (ex: mots de liaisons ou autres): 

```{r}
dfstop <- dfm_select(df, pattern = stopwords(lg), selection = "remove")
```


On applique un filtre de fréquence, on garde que les mots cités au moins 5 fois :

```{r}
dfstop<- dfm_trim(dfstop, min_termfreq = 5)
```


Maintenant on fusionne les tweets des mêmes identifiants :
```{r}
DFM <- dfm_group(dfstop, groups = input[,doc])

```
On le transforme en data frame pour l'analyse plus tard :

```{r}
DFM<-as.data.frame(DFM)
```

Et on clos la fonction et on retourne le DFM construit :

```{r}

rownames(DFM)<-DFM[,1]

DFM<-DFM[,-1]

cat("Finished \n")

return(DFM )
}
```




### __**Estimation des poids et prévision de la target<a class="anchor" id="Estimation des poids et prévision de la target"></a>**__

Nous allons maintenant créer une fonction qui permets de calculer le sous ensemble des mots les plus significatifs afin de calculer la prévision du target. 
Pour cela, on va créer une fonction qui calibre et donc estime les paramètres (betas) et une fonction qui applique ces poids aux utilisateurs pour prédire le target.

#### __**Calibration/Estimation des paramètres<a class="anchor" id="Calibration/Estimation des paramètres"></a>**__



On va calculer les betas sur le data de calibration, pour pouvoir ensuite les fixer pour prédire sur les users.

Cette fonction prends donc en entrée un Document-Features-Matrix converti en data frame, un vecteur qui va nous permettre d'identifier les individus qui calibrent le sous espace, ainsi qu'un argument logique qui permettra de savoir si le DFM est complet et donc qu'il possède les individus sur lesquels on calibre ET les individus sur lesquels on prédit, ou alors s'il n'est pas complet et dans ce cas il possède que les individus sur lesquels on calibre et estime les paramètres.

```{r}
calibrate<- function (
  input,  # a data.frame avec lignes/users et col/text
  complet = T, #T ou F si T data avec calib et users
 # ID,     #si T faut enlever col avec differenciation
  calib, # si c'est T faut diff
  ngram=2,
  wordsinrow=FALSE,
  docincol = FALSE
){
```


On distingue les deux cas complet = F ou T et on estime les paramètres à l'aide de la fonction wordfish :

```{r}
if (complet ==  F){
    data<- input

    ###### trouvons les beta à fixer

    t_data<-t(data) #on transpose pour faire le wordfish et avoir un FDM
    t_data<-as.data.frame(t_data)

    words<-rownames(t_data) # les mots sont les nomes des lignes
    target_name<-colnames(t_data)
```
  
  On applique maintenant les filtres des mots jamais utilisés ou des users fantomes : 
  
```{r}
   
    #On supprime les mots qui ont étés prononcés zéro fois par les target

    zero_word <- (rowSums(t_data>0) > 0)

    t_data_without_zero <- t_data[zero_word, ]
    words_kept <- words[zero_word]

    zero_docs <- (colSums(t_data_without_zero) > 0)
    t_data_without_zero <- t_data_without_zero[, zero_docs]
    target_kept <- target_name[zero_docs]


    sum(rowSums(t_data_without_zero> 0) == 0)
    sum(colSums(t_data_without_zero > 0) == 0)
    dim(t_data_without_zero)
```

On peut appliquer le wordfish et créer le data frame qui contient les mots et leurs poids :

```{r}
   
    wf_out_data <- wordfish(t_data_without_zero, fixtwo = FALSE, dir = c(1, 2), wordsincol = FALSE, tol = 1e-04)

    omega <- wf_out_data$documents[, "omega"]
    print(omega)
    beta <- wf_out_data$words[, "b"]
    psi <- wf_out_data$words[, "psi"]
    print(psi)

    ### associer les mots et leurs poids respectifs

    word_df <- data.frame(words_kept,beta)
    word_top<-word_df[sort(abs(word_df$beta),decreasing=T,index.return=T)[[2]],][1:6,]
    print(word_df)
    return(list(word_df=word_df,words_kept=words_kept,word_top=word_top))

  }
```

Si le DFM contient les individus sur lesquels on veut prédire il faut diviser en deux data et retourner le data des individus ainsi que le data avec le poids des mots pour pouvoir prédire avec la fonction qui va suivre :

```{r}
  else{
 # data_target_name<-data[,ID]#nom du target

  #data_without_target<-data[, !(colnames(data) %in% c(ID)), drop = FALSE]#on enleve le nom du target du data

  #on sépare le data en deux data, un avec les individus qui servent à calibrer (data_target) et l'autre le reste
  data<-input
  data_users<-data[-calib,]

  #rownames(data_users)[0] <- "word" #on donne un nom a la première ligne
  data_target<-data[calib,]
```



On effectue maintenant la même chose que précedement  :


```{r}
  ###### trouvons les beta à fixer

  t_data_target<-t(data_target) #on transpose pour faire le wordfish et avoir un FDM
  t_data_target<-as.data.frame(t_data_target)

  words<-rownames(t_data_target) # les mots sont les nomes des lignes
  target_name<-colnames(t_data_target)

  #On supprime les mots qui ont étés prononcés zéro fois par les target

  zero_word <- (rowSums(t_data_target>0) > 0)

  t_data_target_without_zero <- t_data_target[zero_word, ]

  words_kept <- words[zero_word]

  zero_docs <- (colSums(t_data_target_without_zero>0) > 0)
  t_data_target_without_zero <- t_data_target_without_zero[, zero_docs]
  data_target_without_zero<-t(t_data_target_without_zero)
  target_kept <- target_name[zero_docs]


  sum(rowSums(t_data_target_without_zero> 0) == 0)
  sum(colSums(t_data_target_without_zero > 0) == 0)
  dim(t_data_target_without_zero)


  wf_out <- wordfish(t_data_target_without_zero, fixtwo = FALSE, dir = c(1, 2),  wordsincol = FALSE, tol = 1e-04)

  omega <- wf_out$documents[, "omega"]
  print(omega)
  beta <- wf_out$words[, "b"]
  psi <- wf_out$words[, "psi"]
  opini<-wf_out$documents[,'omega']
  print(psi)
```

On peut maintenant retourner le data des opinions calculés sur les individus qui calibre le sous espace, le data des indiviuds sur lequelle on veut prédire ainsi que le data avec les mots et leurs poids :


```{r}
  ### associer les mots et leurs poids respectifs

  word_df <- data.frame(words_kept,beta)
  library(dplyr)
  word_df<-arrange(word_df,desc(abs(beta)))
  opini_df<-data.frame(rownames(data_target_without_zero),opini)
  colnames(opini_df)<-c("users","opinions")
  word_top<-word_df[sort(abs(word_df$beta),decreasing=T,index.return=T)[[2]],][1:6,]
  print(word_df)
  data_users<-data_users[,words_kept]
  return(list(word_df=word_df,data_users=data_users,opinions=opini_df))
  }

}
```


#### __**Prédiction<a class="anchor" id="Prédiction"></a>**__
 

Maintenant créons la fonction qui utilise les paramètres calculée précédemment et les applique sur les autres individus pour estimer leurs target.

On va fixer les poids et calculons les positions (oméga) sur le deuxième data composé uniquement des individus dont on veut prédire la target.

Pour cela on va refaire le wordfish sur le data avec le bétas fixé (la fonction est modifié de sorte que les bétas ne soit pas recalculés et prennent en entrée le vecteur poids).

Cette fonction prends en entrée le data des indiviuds sur lesquelles il faut prédire, les mots gardées et filtrée par la fonction précédente, le data des mots et de leurs poids respectifs:

```{r}
use_weight<- function (
  input,  # a data.frame avec lignes/users et col/text
  words,   # words kept calculating in the last function
  df,     #word/weight data
  wordsinrow=FALSE,
  docincol = FALSE
){

```
  
  On va effectuer les mêmes filtres que précedement sur les utilisateurs fantomes ainsi que les mots jamais utilisés :
  
```{r}
  data_users<-input
  words_kept<-words
  word_df<-df

  #### fixer les poids

  t_data_users<-t(data_users)#on transpose pour pouvoir assembler les data
  t_data_users<-as.data.frame(t_data_users)

  t_data_users_kept<-t_data_users#on garde que les mots utilisés par les calibreurs
  users_name<-colnames(t_data_users_kept)

  #On enlève ceux qui ont étés utilisées zéro fois par tout le monde

  zero_word1 <- (rowSums(t_data_users_kept>0) > 0)

  t_data_users_without_zero <- t_data_users_kept[zero_word1, ]

  words_kept1 <- words_kept[zero_word1]

  zero_docs1 <- (colSums(t_data_users_without_zero) > 0)
  t_data_users_without_zero <- t_data_users_without_zero[, zero_docs1]
  users_kept1 <- users_name[zero_docs1]

```
  
  On peut maintenant apliquées la fonction wordfish modifiées :
  
  

```{r}
  #on les assemble aux mots

  word_df<-word_df[zero_word1,]
  t_data_users_without_zero[,"weight"]<-word_df[,2]
  t_data_users_without_zero[,"words"]<-word_df[,1]




  wordcountdata_users_weighted <- t_data_users_without_zero

  L <- dim(wordcountdata_users_weighted)[2] # nombre de colonnes

  words_weighted    <- wordcountdata_users_weighted[,L] #les mots
  TM_users_weighted <- wordcountdata_users_weighted[,1:(L-2)]#les occurences
  beta_weighted     <- wordcountdata_users_weighted[,L-1]#les betas

  #on refait le wordfish avec les betas fixés


  sum(rowSums(TM_users_weighted > 0) == 0)
  sum(colSums(TM_users_weighted > 0) == 0)
  dim(TM_users_weighted)



  wf2_out <- wordfish2(beta_weighted,TM_users_weighted,fixtwo=FALSE,dir=c(1,2),wordsincol=FALSE,tol=1e-4)

```

On peut maintenant retourner les opinions calculées et les mots et leurs poids :


```{r}
  beta    <- beta_weighted
  print(beta)
  opinions  <- wf2_out$documents[,'omega']
  words_weighted_df<-data.frame(words_weighted,beta_weighted)
  print(words_weighted_df)
  #opinions_df<-data.frame(rownames(data_users)[0],opinions)
  opinions_df<-data.frame(users_kept1,opinions)
  colnames(opinions_df)<-c("users","opinions")
  return(list(opinions_df=opinions_df,words_weighted_df=words_weighted_df))

}

```




### __**Validation<a class="anchor" id="Validation"></a>**__

Maintenant que nous avons la target prédite nous voulons la comparer au résultat réel pour valider nos calculs.
Pour cela nous allons créer une fonction qui calcule la corrélation de Pearson et la moyenne des écarts relatifs:

```{r}
validation<-function(input_1,input_2){
  
    data_estimation<-input_1
    data_vpl<-input_2
    

    coeff_corr<-cor(data_estimation,data_vpl,method=c("Pearson"))
    
    for(i in 1:nrow(data_estimation)){
      mat_ecart_relatif[i,1]<-(data_vpl[i,1] - data_estimation[i,1])/data_vpl[i,1]
      
    }
  
    
    average_efficiency<-colSums(mat_ecart_relatif[,1])/nrow(mat_ecart_relatif)
    
    return(list(average_efficiency,coeff_corr))
    
  }


```



### __**Optimisation<a class="anchor" id="Optimisation"></a>**__

Maintenant que nous avons ces résultats on peut s'amuser à optimiser le vecteur qui calibre le sous espace pour qu'il renvoie le meilleur sous espace de mots et que donc il prédise au mieux la target et augmente au maximum la métrique de validation.

Pour cela on décide de créer une fonction qui fait 10 sample de 3% de la taille du DFM. Ces 10 samples vont être nos 10 vecteurs de calibration. On va donc pouvoir choisir celui qui maximise la métrique de validation.

On créee donc une fonction qui prends en argument le DFM, le data frame avec les résultats (df validation) ainsi que le nom de la colonne ou est le score dans ce même data :

```{r}
algo_10<- function (
    input,  # a dfm avec lignes/users et col/text
    input2,   # a df with users and target (df validation)
    name_users    # the name of col users in the df validation

  ){

```

On va maintenant créer un data frame qui va contenir les données du vecteur calibration ainsi que le score lorsqu'on calibre sur ce vecteur :

```{r}
data<-input
data_validation<-input2
name_user<-name_users



###########Optimisation

a<-seq(1,10)
data_cor<-data.frame(a)
colnames(data_cor)[1]<-"numéro du sample"
remove(a)

```

On peut maintenant calculer les calibration sur les 10 samples et calculer le score associée et l'écrire dans le data frame :

```{r}
for (i in 1:10){

  X1<-sample(nrow(data), size=round(0.03*(dim(data)[1])))

  for(j in 1:length(X1)){
    data_cor[i,j+1]<-X1[j]
    colnames(data_cor)[j+1]<-"donnée de calib"
  }


  x<-calibrate(data,complet=T,X1)

  data_users<-x[[2]]
  word_df<-x[[1]]
  opini_target<-x[[3]]

  #use weight on the other

  y<-use_weight(data_users,rownames(word_df),word_df)
  opini_users<-y[[1]]
  word_wei<-y[[2]]

  ## Data frame des opinions de tous

  library(dplyr)
  opinions_df<-rbind(opini_target, opini_users)
  opinions_df$users<-as.numeric(opinions_df$users)
  opinions_df$opinions<-as.numeric(opinions_df$opinions)


  ###Validation

  opinions_df_arrange<-arrange(opinions_df,users)

  df_validation_arrange<-arrange(data_validation,name_user)

  op_match<-merge(opinions_df_arrange,df_validation_arrange,by.x = "users",by.y = name_users)

  validation_metrics<-cor(op_match$opinions,op_match[,3])


  data_cor[i,length(X1)+2]<-validation_metrics
  colnames(data_cor)[length(X1)+2]<-"validation_score"

}


return(data_cor=data_cor)
}

```

Nous avons donc en sortie les 10 samples et leurs scores. On peut don choisir celui qui convient.

### __**Annexes<a class="anchor" id="Annexes"></a>**__

Dans cette annexe nous mettons les focntion worfish qui servent à calculer et estimer les poids.

```{r}

wordfish <- function(input,
                     wordsincol = FALSE,
                     fixtwo     = FALSE,
                     dir        = NULL,
                     fixdoc     = c(1, 2, 0, 1),
                     tol        = 1e-07,
                     sigma      = 3,
                     boots      = FALSE,
                     nsim       = 500,
                     writeout   = FALSE,
                     output     = "wordfish_output") {
  dta <- input
  if (wordsincol == TRUE) {
    rownames(dta) <- dta[, 1]
    dta <- dta[, -c(1)]
  }

  dta    <- t(dta)
  words  <- colnames(dta)
  nparty <- nrow(dta)
  nword  <- ncol(dta)

  if (fixtwo == TRUE) {
    if (fixdoc[3] == fixdoc[4]) {
      cat("Warning: fixed omega values in 'fixdoc' cannot be identical. \n")
      stop()
    }

    identprint <- paste("Omegas identified with", rownames(dta)[fixdoc[1]], "=",
                        fixdoc[3], "and ", rownames(dta)[fixdoc[2]], "=", fixdoc[4])
  } else {

    if (sum(c(length(dir) == 2, is.numeric(dir))) != 2) {
      cat("Warning: option 'dir' in wordfish() is empty. You must specify two documents for global identification (e.g. dir=c(1,2) ).\n")
      stop()
    }

    identprint <- paste("Omegas identified with mean 0, st.dev. 1")
  }

  cat("======================================\n")
  cat("WORDFISH (Version 1.3)\n")
  cat("======================================\n")
  cat("Number of unique words: ", nword, "\n")
  cat("Number of documents: ", nparty, "\n")
  cat("Tolerance criterion: ", tol, "\n")
  cat("Identification: ", identprint, "\n")
  cat("======================================\n")

  # Generate starting values ========================

  if (fixtwo == FALSE) {

    rockingstarts <- function(dta) {
      cat("Performing mean 0 sd 1 starting value calc\n")
      P <- nrow(dta)
      W <- ncol(dta)
      numword <- rep(1:W, each = P)
      numparty <- rep(1:P, W)
      dat <- matrix(1, nrow = W * P, ncol = 3)
      dat[, 1] <- as.vector(as.matrix(dta))
      dat[, 2] <- as.vector(numword)
      dat[, 3] <- as.vector(numparty)
      dat <- data.frame(dat)
      colnames(dat) <- c("y", "word", "party")
      dat$word <- factor(dat$word)
      dat$party <- factor(dat$party)

      # Starting values for psi print(dta)
      psi <- log(colMeans(dta))
      # Starting values for alpha
      alpha <- log(rowMeans(dta)/rowMeans(dta)[1])

      # Starting values for beta and x
      ystar <- log(dat$y + 0.1) - alpha[dat$party] - psi[dat$word]
      # print(head(scale(matrix(ystar,nrow(dta),ncol(dta),byrow=FALSE))))
      res <- svd(matrix(ystar, nrow(dta), ncol(dta), byrow = FALSE), nu = 1)
      b <- as.vector(res$v[, 1] * res$d[1])

      omega1 <- as.vector(res$u) - res$u[1, 1]
      omega <- omega1/sd(omega1)
      b <- b * sd(omega1)

      # Create holding bins for some stuff for the convergence code
      min1 <- c(rep(1, nrow(dta) - 1))
      min2 <- c(rep(1, ncol(dta)))
      iter <- 0
      conv <- 0
      diffparam <- 0

      # Put everything together in a list
      list(alpha = as.vector(alpha), psi = as.vector(psi), b = b, omega = omega,
           min1 = min1, min2 = min2, iter = iter, conv = conv, diffparam = diffparam)
    }

  } else {

    rockingstarts <- function(dta, fixval) {
      cat("Performing fix two omega starting value calc\n")
      P <- nrow(dta)
      W <- ncol(dta)
      numword <- rep(1:W, each = P)
      numparty <- rep(1:P, W)
      dat <- matrix(1, nrow = W * P, ncol = 3)
      dat[, 1] <- as.vector(as.matrix(dta))
      dat[, 2] <- as.vector(numword)
      dat[, 3] <- as.vector(numparty)
      dat <- data.frame(dat)
      colnames(dat) <- c("y", "word", "party")
      dat$word <- factor(dat$word)
      dat$party <- factor(dat$party)

      # Starting values for psi
      psi <- log(colMeans(dta))
      # Starting values for alpha
      alpha <- log(rowMeans(dta)/rowMeans(dta)[1])

      # Starting values for beta and x
      ystar <- log(dat$y + 0.1) - alpha[dat$party] - psi[dat$word]
      res <- svd(matrix(ystar, nrow(dta), ncol(dta), byrow = FALSE), nu = 1)
      b <- as.vector(res$v[, 1] * res$d[1])

      omega <- as.vector(res$u)

      # Create holding bins for some stuff for the convergence code
      min1 <- c(rep(1, nrow(dta) - 1))
      min2 <- c(rep(1, ncol(dta)))
      iter <- 0
      conv <- 0
      diffparam <- 0

      # Put everything together in a list
      list(alpha = as.vector(alpha), psi = as.vector(psi), b = b, omega = omega,
           min1 = min1, min2 = min2, iter = iter, conv = conv, diffparam = diffparam)
    }
  }

  # Log-Likelihood Functions (Poisson model)
  # ========================================

  llik_psi_b <- function(p, y, omega, alpha, sigma) {
    # beta and psi will be estimated
    b <- p[1]
    psi <- p[2]
    lambda <- exp(psi + alpha + b * omega)  # Lambda parameter for Poisson distribution
    -(sum(-lambda + log(lambda) * y) - 0.5 * (b^2/sigma^2))  # Log-likelihood including normal prior on Beta
  }


  llik_alpha_1 <- function(p, y, b, psi) {
    # omega[1] is estimated
    omega <- p[1]
    lambda <- exp(psi + b * omega)  # Lambda parameter; alpha is excluded b/c it is set to be zero
    -sum(-lambda + log(lambda) * y)  # Log-likelihood
  }

  llik_alpha_omega <- function(p, y, b, psi) {
    # all other omegas and alphas are estimated
    omega <- p[1]
    alpha <- p[2]
    lambda <- exp(psi + alpha + b * omega)  # Lambda parameter
    -sum(-lambda + log(lambda) * y)  # Log-likelihood
  }


  llik_justalpha <- function(p, y, b, psi, omega) {
    # alpha is estimated
    alpha <- p[1]
    lambda <- exp(psi + alpha + b * omega)  # Lambda parameter
    -sum(-lambda + log(lambda) * y)  # Log-likelihood
  }





  if (fixtwo == FALSE) {


    cat("Performing mean 0 sd 1 EM algorithm\n")
    # Expectation-Maximization Algorithm FOR MEAN 0, SD 1 IDENTIFICATION
    # ==================================================================

    rockingpoisson <- function(dta, tol, sigma, params = NULL, dir = dir, printsum = TRUE) {

      P <- nrow(dta)
      W <- ncol(dta)

      if (is.null(params)) {
        params <- rockingstarts(dta)  # Call up starting value calculation
      }

      iter <- 2
      maxllik <- cbind(-1e+70, rep(0, 1400))
      ll.words <- matrix(-1e+70, W, 1400)
      diffllik <- 500

      # Set the convergence criterion
      conv <- tol
      params$conv <- conv

      while (diffllik > conv) {
        # Run algorithm if difference in LL > convergence criterion
        omegaprev <- params$omega
        bprev <- params$b
        alphaprev <- params$alpha
        psiprev <- params$psi

        # ESTIMATE OMEGA AND ALPHA

        if (printsum == TRUE) {
          cat("Iteration", iter - 1, "\n")
          cat("\tUpdating alpha and omega..\n")
        }



        # Estimate first omega (alpha is set to 0)
        resa <- optim(p = c(params$omega[1]), fn = llik_alpha_1, y = as.numeric(dta[1,
        ]), b = params$b, psi = params$psi, method = c("BFGS"))
        params$omega[1] <- resa$par[1]
        params$min1[1] <- -1 * resa$value
        params$alpha[1] <- 0
        ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
               NA)


        # Estimate all other omegas and alphas
        for (i in 2:P) {

          resa <- optim(par = c(params$omega[i], params$alpha[i]), fn = llik_alpha_omega,
                        y = as.numeric(dta[i, ]), b = params$b, psi = params$psi)
          params$omega[i] <- resa$par[1]
          params$alpha[i] <- resa$par[2]
          params$min1[i] <- -1 * resa$value
          ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                 NA)

        }

        flush.console()


        # Z-score transformation of estimates for omega (to identify model)
        omegabar <- mean(params$omega)
        b1 <- params$b
        params$b <- params$b * sd(params$omega)
        params$omega <- (params$omega - omegabar)/sd(params$omega)
        params$psi <- params$psi + b1 * omegabar

        # Global identification
        if (params$omega[dir[1]] > params$omega[dir[2]]) {
          params$omega <- params$omega * (-1)
        }



        # ESTIMATE PSI AND BETA
        if (printsum == TRUE) {
          cat("\tUpdating psi and beta..\n")
        }

        for (j in 1:W) {
          resb <- optim(par = c(params$b[j], params$psi[j]), fn = llik_psi_b,
                        y = dta[, j], omega = params$omega, alpha = params$alpha, sigma = sigma,lower = -Inf, upper = Inf)
          params$b[j] <- resb$par[1]
          params$psi[j] <- resb$par[2]
          params$min2[j] <- -1 * resb$value
          ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                 NA)
        }

        flush.console()

        # Calculate Log-Likelihood
        maxllik[iter] <- sum(params$min2)
        diffparam <- mean(abs(params$omega - omegaprev))  # difference btw current & previous estimate for omega

        ll.words[, iter] <- params$min2
        diff.ll.words <- (ll.words[, iter] - ll.words[, iter - 1])
        diffllik <- sum(diff.ll.words)/abs(maxllik[iter])


        if (printsum == TRUE) {
          # print(sum(diff.ll.words)) print(abs(maxllik[iter]))
          cat("\tConvergence of LL: ", diffllik, "\n")
        }

        params$diffllik[iter - 1] <- diffllik
        params$diffparam[iter - 1] <- diffparam
        params$diffparam.last <- diffparam
        params$maxllik[iter - 1] <- maxllik[iter]
        params$iter <- iter - 1
        iter <- iter + 1
      }
      params$diffllik[1] <- NA
      return(params)
    }

    # Run the algorithm
    est <- rockingpoisson(dta, tol, sigma, dir = dir)
  } else {
    cat("Performing fix two omega EM algorithm\n")

    # Expectation-Maximization Algorithm FOR TWO FIXED OMEGAS
    # ==================================================================

    rockingpoisson <- function(dta, tol, sigma, params = NULL, fixdoc = fixdoc,
                               printsum = TRUE) {

      P <- nrow(dta)
      W <- ncol(dta)

      if (is.null(params)) {
        params <- rockingstarts(dta, fixval = fixdoc)  # Call up starting value calculation
      }

      iter <- 2
      maxllik <- cbind(-1e+70, rep(0, 1000))
      ll.words <- matrix(-1e+70, W, 1000)

      diffllik <- 500

      # Set the convergence criterion
      conv <- tol
      params$conv <- conv

      while (diffllik > conv) {
        # Run algorithm if difference in LL > convergence criterion
        omegaprev <- params$omega
        bprev <- params$b
        alphaprev <- params$alpha
        psiprev <- params$psi

        # ESTIMATE OMEGA AND ALPHA

        if (printsum == TRUE) {
          cat("Iteration", iter - 1, "\n")
          cat("\tUpdating alpha and omega..\n")
        }


        # Set omegas and first alpha

        params$omega[fixdoc[1]] <- fixdoc[3]
        params$omega[fixdoc[2]] <- fixdoc[4]
        params$alpha[1] <- 0


        if (1 %in% fixdoc[1:2] == TRUE) {

          # if first doc is one of the fixed omegas, do nothing (alpha and omega are fixed)

        } else {
          # Estimate first omega (alpha is set to 0)
          resa <- optim(p = c(params$omega[1]), fn = llik_alpha_1, y = as.numeric(dta[1,
          ]), b = params$b, psi = params$psi, method = c("BFGS"))
          params$omega[1] <- resa$par[1]
          params$min1[1] <- -1 * resa$value
          params$alpha[1] <- 0
          ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                 NA)
        }




        # Estimate all other omegas and alphas
        for (i in 2:P) {


          if (sum(fixdoc[1:2] == i) == 1) {

            # Estimate just alpha
            resa <- optim(par = params$alpha[i], fn = llik_justalpha, y = as.numeric(dta[i,
            ]), b = params$b, psi = params$psi, omega = params$omega[i],
            method = c("BFGS"))
            params$alpha[P] <- resa$par[1]

            ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                   NA)

          } else {
            resa <- optim(par = c(params$omega[i], params$alpha[i]), fn = llik_alpha_omega,
                          y = as.numeric(dta[i, ]), b = params$b, psi = params$psi)
            params$omega[i] <- resa$par[1]
            params$alpha[i] <- resa$par[2]
            params$min1[i] <- -1 * resa$value
            ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                   NA)
          }

        }


        flush.console()



        # ESTIMATE PSI AND BETA
        if (printsum == TRUE) {
          cat("\tUpdating psi and beta..\n")
        }

        for (j in 1:W) {
          resb <- optim(par = c(params$b[j], params$psi[j]), fn = llik_psi_b,
                        y = dta[, j], omega = params$omega, alpha = params$alpha, sigma = sigma)
          params$b[j] <- resb$par[1]
          params$psi[j] <- resb$par[2]
          params$min2[j] <- -1 * resb$value
          ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                 NA)
        }

        flush.console()

        # Calculate Log-Likelihood
        maxllik[iter] <- sum(params$min2)
        diffparam <- mean(abs(params$omega - omegaprev))  # difference between current and previous estimate for omega

        ll.words[, iter] <- params$min2
        diff.ll.words <- (ll.words[, iter] - ll.words[, iter - 1])
        diffllik <- sum(diff.ll.words)/abs(maxllik[iter])

        # print(sum(diff.ll.words)) print(abs(maxllik[iter]))
        if (printsum == TRUE) {
          cat("\tConvergence of LL: ", diffllik, "\n")
        }

        params$diffllik[iter - 1] <- diffllik
        params$diffparam[iter - 1] <- diffparam
        params$diffparam.last <- diffparam
        params$maxllik[iter - 1] <- maxllik[iter]
        params$iter <- iter - 1
        iter <- iter + 1
      }
      params$diffllik[1] <- NA
      return(params)
    }

    # Run the algorithm
    est <- rockingpoisson(dta, tol, sigma, fixdoc = fixdoc)
  }

  cat("======================================\n")
  cat("WORDFISH ML Estimation finished.\n")
  cat("======================================\n\n")

  # Write output
  output.documents <- cbind(est$omega, est$alpha)
  rownames(output.documents) <- rownames(dta)
  colnames(output.documents) <- c("omega", "alpha")
  output.words <- cbind(est$b, est$psi)
  rownames(output.words) <- words
  colnames(output.words) <- c("b", "psi")

  # Write estimation output file Include: Log-likelihood, iterations, number of
  # words, number of documents

  output.estimation <- cbind(nword, nparty, est$iter, sum(est$min2), est$conv,
                             est$diffparam.last)
  colnames(output.estimation) <- c("Words", "Documents", "Iterations", "Log-Likelihood",
                                   "Convergence Criterion", "Difference in X")

  if (writeout == TRUE) {
    write.table(output.documents, file = paste(output, "documents.csv", sep = "_"))
    write.table(output.words, file = paste(output, "words.csv", sep = "_"))
    write.table(output.estimation, file = paste(output, "estimation.csv", sep = "_"))
  }

  ########################### Parametric Bootstrap Code

  bootstrap <- function(nsim, output.documents, output.words, nparty, nword) {

    cat("STARTING PARAMETRIC BOOTSTRAP\n")

    # input alpha and omega from estimation
    alpha.omega <- output.documents

    # input psis and betas from estimation
    psi.beta <- output.words

    # Create matrix of results.
    output.se.omega <- matrix(0, nparty, nsim)
    output.se.b <- matrix(0, nword, nsim)

    alpha <- alpha.omega[, 2]
    omega <- alpha.omega[, 1]
    psi <- psi.beta[, 2]
    b <- psi.beta[, 1]

    # create data matrix
    dtasim <- matrix(1, nrow = nparty, ncol = nword)
    cat("======================================\n")
    cat("Now running", nsim, "bootstrap trials.\n")
    cat("======================================\n")
    cat("Simulation ")

    for (k in 1:nsim) {

      cat(k, "...")

      # Generate new data using lambda
      for (i in 1:nparty) {
        dtasim[i, ] <- rpois(nword, exp(psi + alpha[i] + b * omega[i]))
      }

      alphastart <- alpha + rnorm(length(alpha.omega[, 1]), mean = 0, sd = (sd(alpha.omega[,
                                                                                           2])/2))
      omegastart <- omega + rnorm(length(alpha.omega[, 1]), mean = 0, sd = (sd(alpha.omega[,
                                                                                           1])/2))
      psistart <- psi + rnorm(length(psi.beta[, 1]), mean = 0, sd = (sd(psi.beta[,
                                                                                 2])/2))
      bstart <- b + rnorm(length(psi.beta[, 1]), mean = 0, sd = (sd(psi.beta[,
                                                                             1])/2))
      params <- list(alpha = alphastart, omega = omegastart, psi = psistart,
                     b = bstart)


      if (fixtwo == FALSE) {
        est <- rockingpoisson(dtasim, tol, sigma, params = params, dir = dir,
                              printsum = FALSE)
      } else {
        est <- rockingpoisson(dtasim, tol, sigma, params = params, fixdoc = fixdoc,
                              printsum = FALSE)
      }


      # Store omegas
      output.se.omega[, k] <- est$omega
      # Store Bs
      output.se.b[, k] <- est$b
    }


    conf.documents <- matrix(0, nparty, 4)
    colnames(conf.documents) <- c("LB", "UB", "Omega: ML", "Omega: Sim Mean")
    rownames(conf.documents) <- rownames(dta)
    for (i in 1:nparty) {
      conf.documents[i, 1] <- quantile(output.se.omega[i, ], 0.025)
      conf.documents[i, 2] <- quantile(output.se.omega[i, ], 0.975)
      conf.documents[i, 3] <- omega[i]
      conf.documents[i, 4] <- mean(output.se.omega[i, ])
    }



    # CI for word weights
    conf.words <- matrix(0, nword, 4)
    colnames(conf.words) <- c("LB", "UB", "B: ML", "B: Sim Mean")
    rownames(conf.words) <- words


    for (i in 1:nword) {
      conf.words[i, 1] <- quantile(output.se.b[i, ], 0.025)
      conf.words[i, 2] <- quantile(output.se.b[i, ], 0.975)
      conf.words[i, 3] <- b[i]
      conf.words[i, 4] <- mean(output.se.b[i, ])
    }

    return(list(conf.documents = conf.documents, conf.words = conf.words))
  }

  if (boots == TRUE) {
    bootresult <- bootstrap(nsim, output.documents, output.words, nparty, nword)
    ci.documents <- bootresult$conf.documents
    ci.words <- bootresult$conf.words

    if (writeout == TRUE) {
      write.table(ci.words, file = paste(output, "words_95_ci.csv", sep = "_"))
      write.table(ci.documents, file = paste(output, "documents_95_ci.csv",
                                             sep = "_"))
    }

  }

  if (boots == F) {
    ci.documents <- NULL
    ci.words <- NULL
  }

  cat("Finished!\n")

  return(list(documents = output.documents, words = output.words, diffllik = est$diffllik,
              diffomega = est$diffparam, maxllik = est$maxllik, estimation = output.estimation,
              ci.documents = ci.documents, ci.words = ci.words))


}



wordfish2 <- function(words_weight,
                      input,
                      wordsincol = FALSE,
                      fixtwo     = FALSE,
                      dir        = NULL,
                      fixdoc     = c(1, 2, 0, 1),
                      tol        = 1e-07,
                      sigma      = 3,
                      boots      = FALSE,
                      nsim       = 500,
                      writeout   = FALSE,
                      output     = "wordfish_output") {
  dta <- input
  if (wordsincol == TRUE) {
    rownames(dta) <- dta[, 1]
    dta <- dta[, -c(1)]
  }

  dta    <- t(dta)
  words  <- colnames(dta)
  nparty <- nrow(dta)
  nword  <- ncol(dta)

  if (fixtwo == TRUE) {
    if (fixdoc[3] == fixdoc[4]) {
      cat("Warning: fixed omega values in 'fixdoc' cannot be identical. \n")
      stop()
    }

    identprint <- paste("Omegas identified with", rownames(dta)[fixdoc[1]], "=",
                        fixdoc[3], "and ", rownames(dta)[fixdoc[2]], "=", fixdoc[4])
  } else {

    if (sum(c(length(dir) == 2, is.numeric(dir))) != 2) {
      cat("Warning: option 'dir' in wordfish2() is empty. You must specify two documents for global identification (e.g. dir=c(1,2) ).\n")
      stop()
    }

    identprint <- paste("Omegas identified with mean 0, st.dev. 1")
  }


  cat("======================================\n")
  cat("WORDFISH 2 DLA \n")
  cat("======================================\n")
  cat("Number of unique words: ", nword, "\n")
  cat("Number of documents: ", nparty, "\n")
  cat("Tolerance criterion: ", tol, "\n")
  cat("Identification: ", identprint, "\n")
  cat("======================================\n")

  # Generate starting values ========================

  if (fixtwo == FALSE) {



    rockingstarts <- function(dta) {
      cat("Performing mean 0 sd 1 starting value calc\n")
      P <- nrow(dta)
      W <- ncol(dta)
      numword <- rep(1:W, each = P)
      numparty <- rep(1:P, W)
      dat <- matrix(1, nrow = W * P, ncol = 3)
      dat[, 1] <- as.vector(as.matrix(dta))
      dat[, 2] <- as.vector(numword)
      dat[, 3] <- as.vector(numparty)
      dat <- data.frame(dat)
      colnames(dat) <- c("y", "word", "party")
      dat$word <- factor(dat$word)
      dat$party <- factor(dat$party)

      # Starting values for psi print(dta)
      psi <- log(colMeans(dta))
      # Starting values for alpha
      alpha <- log(rowMeans(dta)/rowMeans(dta)[1])

      # Starting values for beta and x
      ystar <- log(dat$y + 0.1) - alpha[dat$party] - psi[dat$word]
      # print(head(scale(matrix(ystar,nrow(dta),ncol(dta),byrow=FALSE))))
      res <- svd(matrix(ystar, nrow(dta), ncol(dta), byrow = FALSE), nu = 1)
      b <- as.vector(res$v[, 1] * res$d[1])

      omega1 <- as.vector(res$u) - res$u[1, 1]
      omega <- omega1/sd(omega1)
      b <- b * sd(omega1)



      # Create holding bins for some stuff for the convergence code
      min1 <- c(rep(1, nrow(dta) - 1))
      min2 <- c(rep(1, ncol(dta)))
      iter <- 0
      conv <- 0
      diffparam <- 0

      # Put everything together in a list
      list(alpha = as.vector(alpha), psi = as.vector(psi), b = b, omega = omega,
           min1 = min1, min2 = min2, iter = iter, conv = conv, diffparam = diffparam)
    }

  } else {


    rockingstarts <- function(dta, fixval) {
      cat("Performing fix two omega starting value calc\n")
      P <- nrow(dta)
      W <- ncol(dta)
      numword <- rep(1:W, each = P)
      numparty <- rep(1:P, W)
      dat <- matrix(1, nrow = W * P, ncol = 3)
      dat[, 1] <- as.vector(as.matrix(dta))
      dat[, 2] <- as.vector(numword)
      dat[, 3] <- as.vector(numparty)
      dat <- data.frame(dat)
      colnames(dat) <- c("y", "word", "party")
      dat$word <- factor(dat$word)
      dat$party <- factor(dat$party)

      # Starting values for psi
      psi <- log(colMeans(dta))
      # Starting values for alpha
      alpha <- log(rowMeans(dta)/rowMeans(dta)[1])

      # Starting values for beta and x
      ystar <- log(dat$y + 0.1) - alpha[dat$party] - psi[dat$word]
      res <- svd(matrix(ystar, nrow(dta), ncol(dta), byrow = FALSE), nu = 1)
      b <- as.vector(res$v[, 1] * res$d[1])

      omega <- as.vector(res$u)




      # Create holding bins for some stuff for the convergence code
      min1 <- c(rep(1, nrow(dta) - 1))
      min2 <- c(rep(1, ncol(dta)))
      iter <- 0
      conv <- 0
      diffparam <- 0

      # Put everything together in a list
      list(alpha = as.vector(alpha), psi = as.vector(psi), b = b, omega = omega,
           min1 = min1, min2 = min2, iter = iter, conv = conv, diffparam = diffparam)
    }


  }


  # Log-Likelihood Functions (Poisson model)
  # ========================================

  llik_psi_b <- function(p, y, omega, alpha, sigma) {
    # beta and psi will be estimated
    b <- p[1]
    psi <- p[2]
    lambda <- exp(psi + alpha + b * omega)  # Lambda parameter for Poisson distribution
    -(sum(-lambda + log(lambda) * y) - 0.5 * (b^2/sigma^2))  # Log-likelihood including normal prior on Beta
  }


  llik_alpha_1 <- function(p, y, b, psi) {
    # omega[1] is estimated
    omega <- p[1]
    lambda <- exp(psi + b * omega)  # Lambda parameter; alpha is excluded b/c it is set to be zero
    -sum(-lambda + log(lambda) * y)  # Log-likelihood
  }

  llik_alpha_omega <- function(p, y, b, psi) {
    # all other omegas and alphas are estimated
    omega <- p[1]
    alpha <- p[2]
    lambda <- exp(psi + alpha + b * omega)  # Lambda parameter
    -sum(-lambda + log(lambda) * y)  # Log-likelihood
  }


  llik_justalpha <- function(p, y, b, psi, omega) {
    # alpha is estimated
    alpha <- p[1]
    lambda <- exp(psi + alpha + b * omega)  # Lambda parameter
    -sum(-lambda + log(lambda) * y)  # Log-likelihood
  }





  if (fixtwo == FALSE) {


    cat("Performing mean 0 sd 1 EM algorithm\n")
    # Expectation-Maximization Algorithm FOR MEAN 0, SD 1 IDENTIFICATION
    # ==================================================================

    rockingpoisson <- function(dta, tol, sigma, params = NULL, dir = dir, printsum = TRUE) {

      P <- nrow(dta)
      W <- ncol(dta)

      if (is.null(params)) {
        params <- rockingstarts(dta)  # Call up starting value calculation
      }

      iter <- 2
      maxllik <- cbind(-1e+70, rep(0, 1400))
      ll.words <- matrix(-1e+70, W, 1400)
      diffllik <- 500

      # Set the convergence criterion
      conv <- tol
      params$conv <- conv

      while (diffllik > conv) {
        # Run algorithm if difference in LL > convergence criterion
        omegaprev <- params$omega
        bprev <- words_weight
        alphaprev <- params$alpha
        psiprev <- params$psi

        # ESTIMATE OMEGA AND ALPHA

        if (printsum == TRUE) {
          cat("Iteration", iter - 1, "\n")
          cat("\tUpdating alpha and omega..\n")
        }


        # Estimate first omega (alpha is set to 0)
        resa <- optim(p = c(params$omega[1]), fn = llik_alpha_1, y = as.numeric(dta[1,
        ]), b = words_weight, psi = params$psi, method = c("BFGS"))
        params$omega[1] <- resa$par[1]
        params$min1[1] <- -1 * resa$value
        params$alpha[1] <- 0
        ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
               NA)


        # Estimate all other omegas and alphas
        for (i in 2:P) {
          resa <- optim(par = c(params$omega[i], params$alpha[i]), fn = llik_alpha_omega,
                        y = as.numeric(dta[i, ]), b = words_weight, psi = params$psi)
          params$omega[i] <- resa$par[1]
          params$alpha[i] <- resa$par[2]
          params$min1[i] <- -1 * resa$value
          ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                 NA)

        }

        flush.console()


        # Z-score transformation of estimates for omega (to identify model)
        omegabar <- mean(params$omega)
        b1 <- words_weight
        params$omega <- (params$omega - omegabar)/sd(params$omega)
        params$psi <- params$psi + b1 * omegabar

        # Global identification
        if (params$omega[dir[1]] > params$omega[dir[2]]) {
          params$omega <- params$omega * (-1)
        }



        # ESTIMATE PSI AND BETA
        if (printsum == TRUE) {
          cat("\tUpdating psi and beta..\n")
        }

        for (j in 1:W) {
          resb <- optim(par = c(words_weight[j], params$psi[j]), fn = llik_psi_b,
                        y = dta[, j], omega = params$omega, alpha = params$alpha, sigma = sigma,lower = -Inf, upper = Inf)
          params$psi[j] <- resb$par[2]
          params$min2[j] <- -1 * resb$value
          ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                 NA)
        }

        flush.console()

        # Calculate Log-Likelihood
        maxllik[iter] <- sum(params$min2)
        diffparam <- mean(abs(params$omega - omegaprev))  # difference btw current & previous estimate for omega

        ll.words[, iter] <- params$min2
        diff.ll.words <- (ll.words[, iter] - ll.words[, iter - 1])
        diffllik <- sum(diff.ll.words)/abs(maxllik[iter])


        if (printsum == TRUE) {
          # print(sum(diff.ll.words)) print(abs(maxllik[iter]))
          cat("\tConvergence of LL: ", diffllik, "\n")
        }

        params$diffllik[iter - 1] <- diffllik
        params$diffparam[iter - 1] <- diffparam
        params$diffparam.last <- diffparam
        params$maxllik[iter - 1] <- maxllik[iter]
        params$iter <- iter - 1
        iter <- iter + 1
      }
      params$diffllik[1] <- NA
      return(params)
    }

    # Run the algorithm
    est <- rockingpoisson(dta, tol, sigma, dir = dir)
  } else {
    cat("Performing fix two omega EM algorithm\n")

    # Expectation-Maximization Algorithm FOR TWO FIXED OMEGAS
    # ==================================================================

    rockingpoisson <- function(dta, tol, sigma, params = NULL, fixdoc = fixdoc,
                               printsum = TRUE) {

      P <- nrow(dta)
      W <- ncol(dta)

      if (is.null(params)) {
        params <- rockingstarts(dta, fixval = fixdoc)  # Call up starting value calculation
      }

      iter <- 2
      maxllik <- cbind(-1e+70, rep(0, 1000))
      ll.words <- matrix(-1e+70, W, 1000)

      diffllik <- 500

      # Set the convergence criterion
      conv <- tol
      params$conv <- conv

      while (diffllik > conv) {
        # Run algorithm if difference in LL > convergence criterion
        omegaprev <- params$omega
        bprev <- params$b
        alphaprev <- params$alpha
        psiprev <- params$psi

        # ESTIMATE OMEGA AND ALPHA

        if (printsum == TRUE) {
          cat("Iteration", iter - 1, "\n")
          cat("\tUpdating alpha and omega..\n")
        }


        # Set omegas and first alpha

        params$omega[fixdoc[1]] <- fixdoc[3]
        params$omega[fixdoc[2]] <- fixdoc[4]
        params$alpha[1] <- 0


        if (1 %in% fixdoc[1:2] == TRUE) {

          # if first doc is one of the fixed omegas, do nothing (alpha and omega are fixed)

        } else {
          # Estimate first omega (alpha is set to 0)
          resa <- optim(p = c(params$omega[1]), fn = llik_alpha_1, y = as.numeric(dta[1,
          ]), b = params$b, psi = params$psi, method = c("BFGS"))
          params$omega[1] <- resa$par[1]
          params$min1[1] <- -1 * resa$value
          params$alpha[1] <- 0
          ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                 NA)
        }




        # Estimate all other omegas and alphas
        for (i in 2:P) {


          if (sum(fixdoc[1:2] == i) == 1) {

            # Estimate just alpha
            resa <- optim(par = params$alpha[i], fn = llik_justalpha, y = as.numeric(dta[i,
            ]), b = params$b, psi = params$psi, omega = params$omega[i],
            method = c("BFGS"))
            params$alpha[P] <- resa$par[1]

            ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                   NA)

          } else {
            resa <- optim(par = c(params$omega[i], params$alpha[i]), fn = llik_alpha_omega,
                          y = as.numeric(dta[i, ]), b = params$b, psi = params$psi)
            params$omega[i] <- resa$par[1]
            params$alpha[i] <- resa$par[2]
            params$min1[i] <- -1 * resa$value
            ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                   NA)
          }

        }


        flush.console()



        # ESTIMATE PSI AND BETA
        if (printsum == TRUE) {
          cat("\tUpdating psi and beta..\n")
        }

        for (j in 1:W) {
          resb <- optim(par = c(params$b[j], params$psi[j]), fn = llik_psi_b,
                        y = dta[, j], omega = params$omega, alpha = params$alpha, sigma = sigma)
          params$b[j] <- resb$par[1]
          params$psi[j] <- resb$par[2]
          params$min2[j] <- -1 * resb$value
          ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                 NA)
        }

        flush.console()

        # Calculate Log-Likelihood
        maxllik[iter] <- sum(params$min2)
        diffparam <- mean(abs(params$omega - omegaprev))  # difference between current and previous estimate for omega

        ll.words[, iter] <- params$min2
        diff.ll.words <- (ll.words[, iter] - ll.words[, iter - 1])
        diffllik <- sum(diff.ll.words)/abs(maxllik[iter])

        # print(sum(diff.ll.words)) print(abs(maxllik[iter]))
        if (printsum == TRUE) {
          cat("\tConvergence of LL: ", diffllik, "\n")
        }

        params$diffllik[iter - 1] <- diffllik
        params$diffparam[iter - 1] <- diffparam
        params$diffparam.last <- diffparam
        params$maxllik[iter - 1] <- maxllik[iter]
        params$iter <- iter - 1
        iter <- iter + 1
      }
      params$diffllik[1] <- NA
      return(params)
    }

    # Run the algorithm
    est <- rockingpoisson(dta, tol, sigma, fixdoc = fixdoc)
  }

  cat("======================================\n")
  cat("WORDFISH 2 DLA ML Estimation finished.\n")
  cat("======================================\n\n")

  # Write output
  output.documents <- cbind(est$omega, est$alpha)
  rownames(output.documents) <- rownames(dta)
  colnames(output.documents) <- c("omega", "alpha")

  # Write estimation output file Include: Log-likelihood, iterations, number of
  # words, number of documents

  output.estimation <- cbind(nword, nparty, est$iter, sum(est$min2), est$conv,
                             est$diffparam.last)
  colnames(output.estimation) <- c("Words", "Documents", "Iterations", "Log-Likelihood",
                                   "Convergence Criterion", "Difference in X")

  if (writeout == TRUE) {
    write.table(output.documents, file = paste(output, "documents.csv", sep = "_"))
    write.table(output.words, file = paste(output, "words.csv", sep = "_"))
    write.table(output.estimation, file = paste(output, "estimation.csv", sep = "_"))
  }

  ########################### Parametric Bootstrap Code

  bootstrap <- function(nsim, output.documents, output.words, nparty, nword) {

    cat("STARTING PARAMETRIC BOOTSTRAP\n")

    # input alpha and omega from estimation
    alpha.omega <- output.documents

    # input psis and betas from estimation
    psi.beta <- output.words

    # Create matrix of results.
    output.se.omega <- matrix(0, nparty, nsim)
    output.se.b <- matrix(0, nword, nsim)

    alpha <- alpha.omega[, 2]
    omega <- alpha.omega[, 1]
    psi <- psi.beta[, 2]
    b <- psi.beta[, 1]

    # create data matrix
    dtasim <- matrix(1, nrow = nparty, ncol = nword)
    cat("======================================\n")
    cat("Now running", nsim, "bootstrap trials.\n")
    cat("======================================\n")
    cat("Simulation ")

    for (k in 1:nsim) {

      cat(k, "...")

      # Generate new data using lambda
      for (i in 1:nparty) {
        dtasim[i, ] <- rpois(nword, exp(psi + alpha[i] + b * omega[i]))
      }

      alphastart <- alpha + rnorm(length(alpha.omega[, 1]), mean = 0, sd = (sd(alpha.omega[,
                                                                                           2])/2))
      omegastart <- omega + rnorm(length(alpha.omega[, 1]), mean = 0, sd = (sd(alpha.omega[,
                                                                                           1])/2))
      psistart <- psi + rnorm(length(psi.beta[, 1]), mean = 0, sd = (sd(psi.beta[,
                                                                                 2])/2))
      bstart <- b + rnorm(length(psi.beta[, 1]), mean = 0, sd = (sd(psi.beta[,
                                                                             1])/2))
      params <- list(alpha = alphastart, omega = omegastart, psi = psistart,
                     b = bstart)


      if (fixtwo == FALSE) {
        est <- rockingpoisson(dtasim, tol, sigma, params = params, dir = dir,
                              printsum = FALSE)
      } else {
        est <- rockingpoisson(dtasim, tol, sigma, params = params, fixdoc = fixdoc,
                              printsum = FALSE)
      }


      # Store omegas
      output.se.omega[, k] <- est$omega
      # Store Bs
      output.se.b[, k] <- words_weight
    }


    conf.documents <- matrix(0, nparty, 4)
    colnames(conf.documents) <- c("LB", "UB", "Omega: ML", "Omega: Sim Mean")
    rownames(conf.documents) <- rownames(dta)
    for (i in 1:nparty) {
      conf.documents[i, 1] <- quantile(output.se.omega[i, ], 0.025)
      conf.documents[i, 2] <- quantile(output.se.omega[i, ], 0.975)
      conf.documents[i, 3] <- omega[i]
      conf.documents[i, 4] <- mean(output.se.omega[i, ])
    }



    # CI for word weights
    conf.words <- matrix(0, nword, 4)
    colnames(conf.words) <- c("LB", "UB", "B: ML", "B: Sim Mean")
    rownames(conf.words) <- words


    for (i in 1:nword) {
      conf.words[i, 1] <- quantile(output.se.b[i, ], 0.025)
      conf.words[i, 2] <- quantile(output.se.b[i, ], 0.975)
      conf.words[i, 3] <- b[i]
      conf.words[i, 4] <- mean(output.se.b[i, ])
    }

    return(list(conf.documents = conf.documents, conf.words = conf.words))
  }

  if (boots == TRUE) {
    bootresult <- bootstrap(nsim, output.documents, output.words, nparty, nword)
    ci.documents <- bootresult$conf.documents
    ci.words <- bootresult$conf.words

    if (writeout == TRUE) {
      write.table(ci.words, file = paste(output, "words_95_ci.csv", sep = "_"))
      write.table(ci.documents, file = paste(output, "documents_95_ci.csv",
                                             sep = "_"))
    }

  }

  if (boots == F) {
    ci.documents <- NULL
    ci.words <- NULL
  }


  cat("Finished!\n")

  return(list(documents = output.documents, diffllik = est$diffllik, diffomega = est$diffparam,
              maxllik = est$maxllik, estimation = output.estimation, ci.documents = ci.documents,
              ci.words = ci.words))

}



```

