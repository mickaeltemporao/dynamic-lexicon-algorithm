---
title: "comment-utiliser-mon-package"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{comment-utiliser-mon-package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


### Table des matières
* [Data Fixure](# Data Fixure)
* [DFM genration process](# DFM genration process)
* [Estimation des poids et prévision de la target](#Estimation des poids et prévision de la target)
* [Validation](#Validation)
* [Annexes](#Annexes)







### __**Data Fixure<a class="anchor" id="Data Fixure"></a>**__


Maintenant nous allons créer une fonction qui va générer un DFM qui va nous servir comme base initiale pour vérifier nos résultats prochainement.

Nous prenons les mots des discours des présidents américains depuis 2011, soit trois présidents :

```{r}
make_data_fixure<-function(nb_users){
  
library(quanteda)
data_pol<-data_corpus_inaugural %>% corpus_subset(Year > 2011) %>%tokens()#on prends que depuis 2011
data_pol<-dfm(data_pol)# on transforme en dfm
data_pol<-dfm_keep(data_pol, min_nchar = 2)#on garde que les mots de plus de 2 lettres
data_pol<-dfm_remove(data_pol, pattern = stopwords("en"))#on enleve les mots communs (ponctuation, et ..)
data_pol<-as.data.frame(data_pol)#on le transforme en data frame
rownames(data_pol)<-data_pol[,1]#on transforme la première colonne en nom des lignes
data_pol<-data_pol[,-1]


```



Maintenant que nous avons crée un data avec les politiciens, nous allons créer aléatoirement des users et des occurences des mots utilisés par ces utilisateurs :

```{r}
a<-c((1:nb_users)) # colonnes des users et leurs noms


b<-abs(c(rnorm(nb_users, mean=0.4, sd=3))) # occurences qui suit une loi normal
b<-round(b)# on met en valeurs entières

data_users<-data.frame(a,b)# on  transforme en data frame 
data_users<-data_users[,-1]
data_users<-as.data.frame(data_users)

for(i in 2:dim(data_pol)[2]){
  data_users[,i]<-round(abs(c(rnorm(nb_users, mean=0.4, sd=3)))) # on rajoute autant qu'il y a de mots
}


```


Maintenant nous pouvons fusionner les deux datas ( users et politiciens )
```{r}
colnames(data_users)<-colnames(data_pol) # on mets les mêmes noms pour fusionner avec les datas des politiciens
data<-rbind(data_pol, data_users)#fusion
```

Nous allons rajouter une colonne permettant de différencier les politicens des uitilisateurs lambdas, nous allons appeler cette colonne ID, si ID = 1 alors c'est un politiciens.
```{r}

data[,dim(data)[2]+1]<-1



for(i in 1:dim(data)[1]){
  if(i<4){
    data[i,dim(data)[2]]=1
  }
else{data[i,dim(data)[2]]=0}
  }

data[,dim(data)[2]]

colnames(data)[dim(data)[2]] <- "ID"


class(colnames(data))

dfm_fixure<-data# data finale


saveRDS(dfm_fixure,"data/data_fixure.rds")


```

Créons mainteant le data prédiction :

```{r}
########### Creation du Data frame validation

df_validation<-data.frame(rownames(data_users))
colnames(df_validation)<-'users_id'
for(i in 1:dim(df_validation)[1]){
  df_validation[i,2]<-rnorm(1, mean=0.4, sd=3)
}

saveRDS(df_validation,"data/df_validation.rds")

return(list(dfm_fixure=dfm_fixure,df_validation=df_validation))

}
```


### __**DFM genration process<a class="anchor" id="DFM genration process"></a>**__


Nous allons maintenant créer une fonction qui va créer le Documents-Features-Matrix pour les fonctions suivantes. 
Pour cela on suppose que l'utilisateur donne en entrée un data frame avec le texte et les documents ainsi que le numéro des colonnes des textes et colonnes.

```{r}
dfm_generation<-function(
  input, #2 col (text & users )
  text,  #numéro de colonne ou il y a le text
  doc,   #numéro de colonne ou il y a les ids
  lg  # "en", "fr" etc..
  ){
```

On va devoir créer un objet token pour decompter les mots dans le texte, on utilise la fonction du package quanteda :

```{r}
library(quanteda)
library(dplyr)
tok<-tokens(input[,text], remove_punct = TRUE,remove_symbols=TRUE,remove_numbers=TRUE,remove_url=TRUE,remove_separators=TRUE)
```


Maintenant que nous avons l'objet tokens on va pouvoir créer un DFM toujours à l'aide la fonction du même package :

```{r}
df<-dfm(tok)# On le convertit en dfm
```


On enleve les mots inutiles (ex mots de liaisons ou autres): 

```{r}
dfstop <- dfm_select(df, pattern = stopwords(lg), selection = "remove")
```


On garde que les mots plus long que 3 lettres :
```{r}
dfstop<-dfm_keep(dfstop, min_nchar = 3)
```

On peut avec les focntion disponible dans ce package appliqué les filtres utils à la bonn analyse textuelle, notamment garder que les mots cités au moins 5 fois :

```{r}
dfstop<- dfm_trim(dfstop, min_termfreq = 5)
```


Maintenant on fusionne les tweets des mêmes identifiants :
```{r}
DFM <- dfm_group(dfstop, groups = Tweet$user_id)

```
On le transforme en data frame pour l'analyse plus tard :

```{r}
DFM<-as.data.frame(DFM)
```

et on clos la fonction :
```{r}
cat("Finished \n")

return(DFM )
}
```

On aurait pu fusionner les identifiants comme ceci mais cela nécessite plus de temps:

```{r}
#df<-as.data.frame(dfstop) 
#df<-df[,-1]#on enleve la colonne id
#a<-Tweet$user_id
#df[,1491]<-a
#colnames(df)[1491]<-"user_id"

#dfM<-aggregate(df[,1:1490], by=list(user_id=df$user_id), FUN=sum)
```









### __**Estimation des poids et prévision de la target<a class="anchor" id="Estimation des poids et prévision de la target"></a>**__


Nous allons maintenant créer une fonction qui permets de calculer le sous ensemble des mots les plus significatifs afin de calculer la prévision du target. 
Pour cela, on va calculer les betas sur le data de calibration, les fixer pour pouvoir prédire sur les users.
Cette fonction prends donc en entrée un Document-Features-Matrix converti en data frame, un vecteur qui va nous permettre d'identifier les individus qui calibrent le sous espace, ainsi qu'un nom de colonne qui identifie ces utilisateurs:

```{r}
DLA <- function (
  input,  # a data.frame
  ID,    #a name of col
  calib,  # a vector of rows
  ngram=2, 
  wordsinrow=FALSE, 
  docincol = FALSE
  ){
    
  #docsincol TRUE si les id/users sont la première colonne dans ce cas faut faire autrement
  #wordsinrow TRUE  si les mots sont la première ligne
  #On a considerer que les documents et features sont les noms des lignes colonnes
```


On va maintenant identifier et supprimer la colonne d'identification des utilisateurs :
```{r}
data<- input
  data_target_name<-data[,ID]#nom du target
 
  data_without_target<-data[, !(colnames(data) %in% c(ID)), drop = FALSE]#on enleve la colonne nom du target du data
```

  
On va maintenant séparer le data en deux datas, un qui comprend les individus qui vont calibrer l'espace et un autre sur lequel on va prédire le target : 

```{r}
data_users<-data_without_target[-calib,]# data sur lequel on fait la prévision


  data_target<-data_without_target[calib,]#data sur lequel on effectue la calibration

```

 
Utilisons la fonction wordfish pour trouver les betas (poids) des mots sur l'ensemble des individus dont on connait la target, donc eux qui calibrent (ici les politiciens):
```{r}
  
###### trouvons les beta à fixer
  
  t_data_target<-t(data_target) #on transpose pour faire le wordfish et avoir un FDM
  t_data_target<-as.data.frame(t_data_target)# on le remets en data frame 
  words<-rownames(t_data_target) # les mots sont les noms des lignes
  target_name<-colnames(t_data_target)
  
    #On supprime les mots qui ont étés prononcés zéro fois par les target
  
  zero_word <- (rowSums(t_data_target>0) > 0)
  
  t_data_target_without_zero <- t_data_target[zero_word, ]
  words_kept <- words[zero_word]
  
  zero_docs <- (colSums(t_data_target_without_zero) > 0)
  t_data_target_without_zero <- t_data_target_without_zero[, zero_docs]
  target_kept <- target_name[zero_docs]
  
  
  sum(rowSums(t_data_target_without_zero> 0) == 0)
  sum(colSums(t_data_target_without_zero > 0) == 0)
  dim(t_data_target_without_zero)
  
  wf_out <- wordfish(t_data_target, fixtwo = FALSE, dir = c(1, 2), wordsincol = FALSE, tol = 1e-04)# on fait la fonction 
  omega <- wf_out$documents[, "omega"]# les omégas sont les positions estimé des individus
  print(omega)
  beta <- wf_out$words[, "b"]# les betas sont les poids des mots
  psi <- wf_out$words[, "psi"]# les psi sont les effets des mots (pas interessant dans notre cas)
  print(psi)
```

Maintenant créons un data frame avec les mots et leurs poids associés :
```{r}
### associer les mots et leurs poids respectifs  
  
  word_df <- data.frame(words_kept,beta)
  print(word_df)
```

Fixons maintenant les poids et calculons les positions (oméga) sur le deuxième data composé uniquement des individus dont on veut prédire la target.
Pour cela on va refaire le wordfish sur le data avec le bétas fixé (la fonction est modifié de sorte que les bétas ne soit pas recalculés et prennent en entrée le vecteur poids).
```{r}
#### fixer les poids
  t_data_users<-t(data_users)#on transpose pour pouvoir assembler les data
  t_data_users<-as.data.frame(t_data_users)
  
  
    t_data_users_kept<-t_data_users[words_kept,]#on garde que les mots utilisés par les calibreurs
  users_name<-colnames(t_data_users_kept)
  
  #On enlève ceux qui ont étés utilisées zéro fois par tout le monde
  
  zero_word1 <- (rowSums(t_data_users_kept>0) > 0)
  
  t_data_users_without_zero <- t_data_users_kept[zero_word1, ]
  
  words_kept1 <- words_kept[zero_word1]
  
  zero_docs1 <- (colSums(t_data_users_without_zero) > 0)
  t_data_users_without_zero <- t_data_users_without_zero[, zero_docs1]
  users_kept1 <- users_name[zero_docs1]
  
  
  #on les assemble aux mots
  
  t_data_users_without_zero[,"weight"]<-word_df[,2]
  t_data_users_without_zero[,"words"]<-word_df[,1]
  
  
  
  
  wordcountdata_users_weighted <- t_data_users_without_zero 
  
  L <- dim(wordcountdata_users_weighted)[2] # nombre de colonnes
  
  words_weighted    <- wordcountdata_users_weighted[,L] #les mots
  TM_users_weighted <- wordcountdata_users_weighted[,1:(L-2)]#les occurences
  beta_weighted     <- wordcountdata_users_weighted[,L-1]#les betas
```


On peut faire le wordfish sur ce data : 
```{r}
  #on refait le wordfish avec les betas fixés
  
  
  sum(rowSums(TM_users_weighted > 0) == 0)
  sum(colSums(TM_users_weighted > 0) == 0)
  dim(TM_users_weighted)
  
    wf2_out <- wordfish2(beta_weighted,TM_users_weighted,fixtwo=FALSE,dir=c(1,2),wordsincol=FALSE,tol=1e-4)
    


    beta    <- beta_weighted
    print(beta)
    opinions  <- wf2_out$documents[,'omega']
  
```

On peut faire un tableau avec les opinions prédit des utilisateurs ainsi que les poids des mots associés:

```{r}
  words_weighted_df<-data.frame(words_weighted,beta_weighted)
  print(words_weighted_df)
  #opinions_df<-data.frame(rownames(data_users)[0],opinions)
  opinions_df<-data.frame(users_kept1,opinions)
  return(list(opinions_df=opinions_df,words_weighted_df=words_weighted_df))
  

}

```

On peut maintenant essayer cette fonction sur notre data fixure : 
```{r}
calib<-c(1,2,3)
ID<-"ID"


DLA(data, "ID", c(1,2,3),ngram=2, 
    wordsinrow=FALSE, 
    docincol = FALSE
)

```

    
   

### __**Validation<a class="anchor" id="Validation"></a>**__

Maintenant que nous avons la target prédite nous voulons la comparer au résultat réel pour valider nos calculs.
Pour cela nous allons calculer la corrélation de Pearson et la moyenne des écarts relatifs:

```{r}
validation<-function(input_1,input_2){
  
    data_estimation<-input_1
    data_vpl<-input_2
    

    coeff_corr<-cor(data_estimation,data_vpl,method=c("Pearson"))
    
    for(i in 1:nrow(data_estimation)){
      mat_ecart_relatif[i,1]<-(data_vpl[i,1] - data_estimation[i,1])/data_vpl[i,1]
      
    }
  
    
    average_efficiency<-colSums(mat_ecart_relatif[,1])/nrow(mat_ecart_relatif)
    
    return(list(average_efficiency,coeff_corr))
    
  }


```

### __**Annexes<a class="anchor" id="Annexes"></a>**__

```{r}
wordfish <- function(input,
                     wordsincol = FALSE,
                     fixtwo     = FALSE,
                     dir        = NULL,
                     fixdoc     = c(1, 2, 0, 1),
                     tol        = 1e-07,
                     sigma      = 3,
                     boots      = FALSE,
                     nsim       = 500,
                     writeout   = FALSE,
                     output     = "wordfish_output") {
  dta <- input
  if (wordsincol == TRUE) {
    rownames(dta) <- dta[, 1]
    dta <- dta[, -c(1)]
  }
  
  dta    <- t(dta)
  words  <- colnames(dta)
  nparty <- nrow(dta)
  nword  <- ncol(dta)
  
  if (fixtwo == TRUE) {
    if (fixdoc[3] == fixdoc[4]) {
      cat("Warning: fixed omega values in 'fixdoc' cannot be identical. \n")
      stop()
    }
    
    identprint <- paste("Omegas identified with", rownames(dta)[fixdoc[1]], "=",
                        fixdoc[3], "and ", rownames(dta)[fixdoc[2]], "=", fixdoc[4])
  } else {
    
    if (sum(c(length(dir) == 2, is.numeric(dir))) != 2) {
      cat("Warning: option 'dir' in wordfish() is empty. You must specify two documents for global identification (e.g. dir=c(1,2) ).\n")
      stop()
    }
    
    identprint <- paste("Omegas identified with mean 0, st.dev. 1")
  }
  
  cat("======================================\n")
  cat("WORDFISH (Version 1.3)\n")
  cat("======================================\n")
  cat("Number of unique words: ", nword, "\n")
  cat("Number of documents: ", nparty, "\n")
  cat("Tolerance criterion: ", tol, "\n")
  cat("Identification: ", identprint, "\n")
  cat("======================================\n")
  
  # Generate starting values ========================
  
  if (fixtwo == FALSE) {
    
    rockingstarts <- function(dta) {
      cat("Performing mean 0 sd 1 starting value calc\n")
      P <- nrow(dta)
      W <- ncol(dta)
      numword <- rep(1:W, each = P)
      numparty <- rep(1:P, W)
      dat <- matrix(1, nrow = W * P, ncol = 3)
      dat[, 1] <- as.vector(as.matrix(dta))
      dat[, 2] <- as.vector(numword)
      dat[, 3] <- as.vector(numparty)
      dat <- data.frame(dat)
      colnames(dat) <- c("y", "word", "party")
      dat$word <- factor(dat$word)
      dat$party <- factor(dat$party)
      
      # Starting values for psi print(dta)
      psi <- log(colMeans(dta))
      # Starting values for alpha
      alpha <- log(rowMeans(dta)/rowMeans(dta)[1])
      
      # Starting values for beta and x
      ystar <- log(dat$y + 0.1) - alpha[dat$party] - psi[dat$word]
      # print(head(scale(matrix(ystar,nrow(dta),ncol(dta),byrow=FALSE))))
      res <- svd(matrix(ystar, nrow(dta), ncol(dta), byrow = FALSE), nu = 1)
      b <- as.vector(res$v[, 1] * res$d[1])
      
      omega1 <- as.vector(res$u) - res$u[1, 1]
      omega <- omega1/sd(omega1)
      b <- b * sd(omega1)
      
      # Create holding bins for some stuff for the convergence code
      min1 <- c(rep(1, nrow(dta) - 1))
      min2 <- c(rep(1, ncol(dta)))
      iter <- 0
      conv <- 0
      diffparam <- 0
      
      # Put everything together in a list
      list(alpha = as.vector(alpha), psi = as.vector(psi), b = b, omega = omega,
           min1 = min1, min2 = min2, iter = iter, conv = conv, diffparam = diffparam)
    }
    
  } else {
    
    rockingstarts <- function(dta, fixval) {
      cat("Performing fix two omega starting value calc\n")
      P <- nrow(dta)
      W <- ncol(dta)
      numword <- rep(1:W, each = P)
      numparty <- rep(1:P, W)
      dat <- matrix(1, nrow = W * P, ncol = 3)
      dat[, 1] <- as.vector(as.matrix(dta))
      dat[, 2] <- as.vector(numword)
      dat[, 3] <- as.vector(numparty)
      dat <- data.frame(dat)
      colnames(dat) <- c("y", "word", "party")
      dat$word <- factor(dat$word)
      dat$party <- factor(dat$party)
      
      # Starting values for psi
      psi <- log(colMeans(dta))
      # Starting values for alpha
      alpha <- log(rowMeans(dta)/rowMeans(dta)[1])
      
      # Starting values for beta and x
      ystar <- log(dat$y + 0.1) - alpha[dat$party] - psi[dat$word]
      res <- svd(matrix(ystar, nrow(dta), ncol(dta), byrow = FALSE), nu = 1)
      b <- as.vector(res$v[, 1] * res$d[1])
      
      omega <- as.vector(res$u)
      
      # Create holding bins for some stuff for the convergence code
      min1 <- c(rep(1, nrow(dta) - 1))
      min2 <- c(rep(1, ncol(dta)))
      iter <- 0
      conv <- 0
      diffparam <- 0
      
      # Put everything together in a list
      list(alpha = as.vector(alpha), psi = as.vector(psi), b = b, omega = omega,
           min1 = min1, min2 = min2, iter = iter, conv = conv, diffparam = diffparam)
    }
  }
  
  # Log-Likelihood Functions (Poisson model)
  # ========================================
  
  llik_psi_b <- function(p, y, omega, alpha, sigma) {
    # beta and psi will be estimated
    b <- p[1]
    psi <- p[2]
    lambda <- exp(psi + alpha + b * omega)  # Lambda parameter for Poisson distribution
    -(sum(-lambda + log(lambda) * y) - 0.5 * (b^2/sigma^2))  # Log-likelihood including normal prior on Beta
  }
  
  
  llik_alpha_1 <- function(p, y, b, psi) {
    # omega[1] is estimated
    omega <- p[1]
    lambda <- exp(psi + b * omega)  # Lambda parameter; alpha is excluded b/c it is set to be zero
    -sum(-lambda + log(lambda) * y)  # Log-likelihood
  }
  
  llik_alpha_omega <- function(p, y, b, psi) {
    # all other omegas and alphas are estimated
    omega <- p[1]
    alpha <- p[2]
    lambda <- exp(psi + alpha + b * omega)  # Lambda parameter
    -sum(-lambda + log(lambda) * y)  # Log-likelihood
  }
  
  
  llik_justalpha <- function(p, y, b, psi, omega) {
    # alpha is estimated
    alpha <- p[1]
    lambda <- exp(psi + alpha + b * omega)  # Lambda parameter
    -sum(-lambda + log(lambda) * y)  # Log-likelihood
  }
  
  
  
  
  
  if (fixtwo == FALSE) {
    
    
    cat("Performing mean 0 sd 1 EM algorithm\n")
    # Expectation-Maximization Algorithm FOR MEAN 0, SD 1 IDENTIFICATION
    # ==================================================================
    
    rockingpoisson <- function(dta, tol, sigma, params = NULL, dir = dir, printsum = TRUE) {
      
      P <- nrow(dta)
      W <- ncol(dta)
      
      if (is.null(params)) {
        params <- rockingstarts(dta)  # Call up starting value calculation
      }
      
      iter <- 2
      maxllik <- cbind(-1e+70, rep(0, 1400))
      ll.words <- matrix(-1e+70, W, 1400)
      diffllik <- 500
      
      # Set the convergence criterion
      conv <- tol
      params$conv <- conv
      
      while (diffllik > conv) {
        # Run algorithm if difference in LL > convergence criterion
        omegaprev <- params$omega
        bprev <- params$b
        alphaprev <- params$alpha
        psiprev <- params$psi
        
        # ESTIMATE OMEGA AND ALPHA
        
        if (printsum == TRUE) {
          cat("Iteration", iter - 1, "\n")
          cat("\tUpdating alpha and omega..\n")
        }
        
        
        
        # Estimate first omega (alpha is set to 0)
        resa <- optim(p = c(params$omega[1]), fn = llik_alpha_1, y = as.numeric(dta[1,
        ]), b = params$b, psi = params$psi, method = c("BFGS"))
        params$omega[1] <- resa$par[1]
        params$min1[1] <- -1 * resa$value
        params$alpha[1] <- 0
        ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
               NA)
        
        
        # Estimate all other omegas and alphas
        for (i in 2:P) {
          
          resa <- optim(par = c(params$omega[i], params$alpha[i]), fn = llik_alpha_omega,
                        y = as.numeric(dta[i, ]), b = params$b, psi = params$psi)
          params$omega[i] <- resa$par[1]
          params$alpha[i] <- resa$par[2]
          params$min1[i] <- -1 * resa$value
          ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                 NA)
          
        }
        
        flush.console()
        
        
        # Z-score transformation of estimates for omega (to identify model)
        omegabar <- mean(params$omega)
        b1 <- params$b
        params$b <- params$b * sd(params$omega)
        params$omega <- (params$omega - omegabar)/sd(params$omega)
        params$psi <- params$psi + b1 * omegabar
        
        # Global identification
        if (params$omega[dir[1]] > params$omega[dir[2]]) {
          params$omega <- params$omega * (-1)
        }
        
        
        
        # ESTIMATE PSI AND BETA
        if (printsum == TRUE) {
          cat("\tUpdating psi and beta..\n")
        }
        
        for (j in 1:W) {
          resb <- optim(par = c(params$b[j], params$psi[j]), fn = llik_psi_b,
                        y = dta[, j], omega = params$omega, alpha = params$alpha, sigma = sigma)
          params$b[j] <- resb$par[1]
          params$psi[j] <- resb$par[2]
          params$min2[j] <- -1 * resb$value
          ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                 NA)
        }
        
        flush.console()
        
        # Calculate Log-Likelihood
        maxllik[iter] <- sum(params$min2)
        diffparam <- mean(abs(params$omega - omegaprev))  # difference btw current & previous estimate for omega
        
        ll.words[, iter] <- params$min2
        diff.ll.words <- (ll.words[, iter] - ll.words[, iter - 1])
        diffllik <- sum(diff.ll.words)/abs(maxllik[iter])
        
        
        if (printsum == TRUE) {
          # print(sum(diff.ll.words)) print(abs(maxllik[iter]))
          cat("\tConvergence of LL: ", diffllik, "\n")
        }
        
        params$diffllik[iter - 1] <- diffllik
        params$diffparam[iter - 1] <- diffparam
        params$diffparam.last <- diffparam
        params$maxllik[iter - 1] <- maxllik[iter]
        params$iter <- iter - 1
        iter <- iter + 1
      }
      params$diffllik[1] <- NA
      return(params)
    }
    
    # Run the algorithm
    est <- rockingpoisson(dta, tol, sigma, dir = dir)
  } else {
    cat("Performing fix two omega EM algorithm\n")
    
    # Expectation-Maximization Algorithm FOR TWO FIXED OMEGAS
    # ==================================================================
    
    rockingpoisson <- function(dta, tol, sigma, params = NULL, fixdoc = fixdoc,
                               printsum = TRUE) {
      
      P <- nrow(dta)
      W <- ncol(dta)
      
      if (is.null(params)) {
        params <- rockingstarts(dta, fixval = fixdoc)  # Call up starting value calculation
      }
      
      iter <- 2
      maxllik <- cbind(-1e+70, rep(0, 1000))
      ll.words <- matrix(-1e+70, W, 1000)
      
      diffllik <- 500
      
      # Set the convergence criterion
      conv <- tol
      params$conv <- conv
      
      while (diffllik > conv) {
        # Run algorithm if difference in LL > convergence criterion
        omegaprev <- params$omega
        bprev <- params$b
        alphaprev <- params$alpha
        psiprev <- params$psi
        
        # ESTIMATE OMEGA AND ALPHA
        
        if (printsum == TRUE) {
          cat("Iteration", iter - 1, "\n")
          cat("\tUpdating alpha and omega..\n")
        }
        
        
        # Set omegas and first alpha
        
        params$omega[fixdoc[1]] <- fixdoc[3]
        params$omega[fixdoc[2]] <- fixdoc[4]
        params$alpha[1] <- 0
        
        
        if (1 %in% fixdoc[1:2] == TRUE) {
          
          # if first doc is one of the fixed omegas, do nothing (alpha and omega are fixed)
          
        } else {
          # Estimate first omega (alpha is set to 0)
          resa <- optim(p = c(params$omega[1]), fn = llik_alpha_1, y = as.numeric(dta[1,
          ]), b = params$b, psi = params$psi, method = c("BFGS"))
          params$omega[1] <- resa$par[1]
          params$min1[1] <- -1 * resa$value
          params$alpha[1] <- 0
          ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                 NA)
        }
        
        
        
        
        # Estimate all other omegas and alphas
        for (i in 2:P) {
          
          
          if (sum(fixdoc[1:2] == i) == 1) {
            
            # Estimate just alpha
            resa <- optim(par = params$alpha[i], fn = llik_justalpha, y = as.numeric(dta[i,
            ]), b = params$b, psi = params$psi, omega = params$omega[i],
            method = c("BFGS"))
            params$alpha[P] <- resa$par[1]
            
            ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                   NA)
            
          } else {
            resa <- optim(par = c(params$omega[i], params$alpha[i]), fn = llik_alpha_omega,
                          y = as.numeric(dta[i, ]), b = params$b, psi = params$psi)
            params$omega[i] <- resa$par[1]
            params$alpha[i] <- resa$par[2]
            params$min1[i] <- -1 * resa$value
            ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                   NA)
          }
          
        }
        
        
        flush.console()
        
        
        
        # ESTIMATE PSI AND BETA
        if (printsum == TRUE) {
          cat("\tUpdating psi and beta..\n")
        }
        
        for (j in 1:W) {
          resb <- optim(par = c(params$b[j], params$psi[j]), fn = llik_psi_b,
                        y = dta[, j], omega = params$omega, alpha = params$alpha, sigma = sigma)
          params$b[j] <- resb$par[1]
          params$psi[j] <- resb$par[2]
          params$min2[j] <- -1 * resb$value
          ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                 NA)
        }
        
        flush.console()
        
        # Calculate Log-Likelihood
        maxllik[iter] <- sum(params$min2)
        diffparam <- mean(abs(params$omega - omegaprev))  # difference between current and previous estimate for omega
        
        ll.words[, iter] <- params$min2
        diff.ll.words <- (ll.words[, iter] - ll.words[, iter - 1])
        diffllik <- sum(diff.ll.words)/abs(maxllik[iter])
        
        # print(sum(diff.ll.words)) print(abs(maxllik[iter]))
        if (printsum == TRUE) {
          cat("\tConvergence of LL: ", diffllik, "\n")
        }
        
        params$diffllik[iter - 1] <- diffllik
        params$diffparam[iter - 1] <- diffparam
        params$diffparam.last <- diffparam
        params$maxllik[iter - 1] <- maxllik[iter]
        params$iter <- iter - 1
        iter <- iter + 1
      }
      params$diffllik[1] <- NA
      return(params)
    }
    
    # Run the algorithm
    est <- rockingpoisson(dta, tol, sigma, fixdoc = fixdoc)
  }
  
  cat("======================================\n")
  cat("WORDFISH ML Estimation finished.\n")
  cat("======================================\n\n")
  
  # Write output
  output.documents <- cbind(est$omega, est$alpha)
  rownames(output.documents) <- rownames(dta)
  colnames(output.documents) <- c("omega", "alpha")
  output.words <- cbind(est$b, est$psi)
  rownames(output.words) <- words
  colnames(output.words) <- c("b", "psi")
  
  # Write estimation output file Include: Log-likelihood, iterations, number of
  # words, number of documents
  
  output.estimation <- cbind(nword, nparty, est$iter, sum(est$min2), est$conv,
                             est$diffparam.last)
  colnames(output.estimation) <- c("Words", "Documents", "Iterations", "Log-Likelihood",
                                   "Convergence Criterion", "Difference in X")
  
  if (writeout == TRUE) {
    write.table(output.documents, file = paste(output, "documents.csv", sep = "_"))
    write.table(output.words, file = paste(output, "words.csv", sep = "_"))
    write.table(output.estimation, file = paste(output, "estimation.csv", sep = "_"))
  }
  
  ########################### Parametric Bootstrap Code
  
  bootstrap <- function(nsim, output.documents, output.words, nparty, nword) {
    
    cat("STARTING PARAMETRIC BOOTSTRAP\n")
    
    # input alpha and omega from estimation
    alpha.omega <- output.documents
    
    # input psis and betas from estimation
    psi.beta <- output.words
    
    # Create matrix of results.
    output.se.omega <- matrix(0, nparty, nsim)
    output.se.b <- matrix(0, nword, nsim)
    
    alpha <- alpha.omega[, 2]
    omega <- alpha.omega[, 1]
    psi <- psi.beta[, 2]
    b <- psi.beta[, 1]
    
    # create data matrix
    dtasim <- matrix(1, nrow = nparty, ncol = nword)
    cat("======================================\n")
    cat("Now running", nsim, "bootstrap trials.\n")
    cat("======================================\n")
    cat("Simulation ")
    
    for (k in 1:nsim) {
      
      cat(k, "...")
      
      # Generate new data using lambda
      for (i in 1:nparty) {
        dtasim[i, ] <- rpois(nword, exp(psi + alpha[i] + b * omega[i]))
      }
      
      alphastart <- alpha + rnorm(length(alpha.omega[, 1]), mean = 0, sd = (sd(alpha.omega[,
                                                                                           2])/2))
      omegastart <- omega + rnorm(length(alpha.omega[, 1]), mean = 0, sd = (sd(alpha.omega[,
                                                                                           1])/2))
      psistart <- psi + rnorm(length(psi.beta[, 1]), mean = 0, sd = (sd(psi.beta[,
                                                                                 2])/2))
      bstart <- b + rnorm(length(psi.beta[, 1]), mean = 0, sd = (sd(psi.beta[,
                                                                             1])/2))
      params <- list(alpha = alphastart, omega = omegastart, psi = psistart,
                     b = bstart)
      
      
      if (fixtwo == FALSE) {
        est <- rockingpoisson(dtasim, tol, sigma, params = params, dir = dir,
                              printsum = FALSE)
      } else {
        est <- rockingpoisson(dtasim, tol, sigma, params = params, fixdoc = fixdoc,
                              printsum = FALSE)
      }
      
      
      # Store omegas
      output.se.omega[, k] <- est$omega
      # Store Bs
      output.se.b[, k] <- est$b
    }
    
    
    conf.documents <- matrix(0, nparty, 4)
    colnames(conf.documents) <- c("LB", "UB", "Omega: ML", "Omega: Sim Mean")
    rownames(conf.documents) <- rownames(dta)
    for (i in 1:nparty) {
      conf.documents[i, 1] <- quantile(output.se.omega[i, ], 0.025)
      conf.documents[i, 2] <- quantile(output.se.omega[i, ], 0.975)
      conf.documents[i, 3] <- omega[i]
      conf.documents[i, 4] <- mean(output.se.omega[i, ])
    }
    
    
    
    # CI for word weights
    conf.words <- matrix(0, nword, 4)
    colnames(conf.words) <- c("LB", "UB", "B: ML", "B: Sim Mean")
    rownames(conf.words) <- words
    
    
    for (i in 1:nword) {
      conf.words[i, 1] <- quantile(output.se.b[i, ], 0.025)
      conf.words[i, 2] <- quantile(output.se.b[i, ], 0.975)
      conf.words[i, 3] <- b[i]
      conf.words[i, 4] <- mean(output.se.b[i, ])
    }
    
    return(list(conf.documents = conf.documents, conf.words = conf.words))
  }
  
  if (boots == TRUE) {
    bootresult <- bootstrap(nsim, output.documents, output.words, nparty, nword)
    ci.documents <- bootresult$conf.documents
    ci.words <- bootresult$conf.words
    
    if (writeout == TRUE) {
      write.table(ci.words, file = paste(output, "words_95_ci.csv", sep = "_"))
      write.table(ci.documents, file = paste(output, "documents_95_ci.csv",
                                             sep = "_"))
    }
    
  }
  
  if (boots == F) {
    ci.documents <- NULL
    ci.words <- NULL
  }
  
  cat("Finished!\n")
  
  return(list(documents = output.documents, words = output.words, diffllik = est$diffllik,
              diffomega = est$diffparam, maxllik = est$maxllik, estimation = output.estimation,
              ci.documents = ci.documents, ci.words = ci.words))
  
  
}



wordfish2 <- function(words_weight,
                      input,
                      wordsincol = FALSE,
                      fixtwo     = FALSE,
                      dir        = NULL,
                      fixdoc     = c(1, 2, 0, 1),
                      tol        = 1e-07,
                      sigma      = 3,
                      boots      = FALSE,
                      nsim       = 500,
                      writeout   = FALSE,
                      output     = "wordfish_output") {
  dta <- input
  if (wordsincol == TRUE) {
    rownames(dta) <- dta[, 1]
    dta <- dta[, -c(1)]
  }
  
  dta    <- t(dta)
  words  <- colnames(dta)
  nparty <- nrow(dta)
  nword  <- ncol(dta)
  
  if (fixtwo == TRUE) {
    if (fixdoc[3] == fixdoc[4]) {
      cat("Warning: fixed omega values in 'fixdoc' cannot be identical. \n")
      stop()
    }
    
    identprint <- paste("Omegas identified with", rownames(dta)[fixdoc[1]], "=",
                        fixdoc[3], "and ", rownames(dta)[fixdoc[2]], "=", fixdoc[4])
  } else {
    
    if (sum(c(length(dir) == 2, is.numeric(dir))) != 2) {
      cat("Warning: option 'dir' in wordfish2() is empty. You must specify two documents for global identification (e.g. dir=c(1,2) ).\n")
      stop()
    }
    
    identprint <- paste("Omegas identified with mean 0, st.dev. 1")
  }
  
  
  cat("======================================\n")
  cat("WORDFISH 2 DLA \n")
  cat("======================================\n")
  cat("Number of unique words: ", nword, "\n")
  cat("Number of documents: ", nparty, "\n")
  cat("Tolerance criterion: ", tol, "\n")
  cat("Identification: ", identprint, "\n")
  cat("======================================\n")
  
  # Generate starting values ========================
  
  if (fixtwo == FALSE) {
    
    
    
    rockingstarts <- function(dta) {
      cat("Performing mean 0 sd 1 starting value calc\n")
      P <- nrow(dta)
      W <- ncol(dta)
      numword <- rep(1:W, each = P)
      numparty <- rep(1:P, W)
      dat <- matrix(1, nrow = W * P, ncol = 3)
      dat[, 1] <- as.vector(as.matrix(dta))
      dat[, 2] <- as.vector(numword)
      dat[, 3] <- as.vector(numparty)
      dat <- data.frame(dat)
      colnames(dat) <- c("y", "word", "party")
      dat$word <- factor(dat$word)
      dat$party <- factor(dat$party)
      
      # Starting values for psi print(dta)
      psi <- log(colMeans(dta))
      # Starting values for alpha
      alpha <- log(rowMeans(dta)/rowMeans(dta)[1])
      
      # Starting values for beta and x
      ystar <- log(dat$y + 0.1) - alpha[dat$party] - psi[dat$word]
      # print(head(scale(matrix(ystar,nrow(dta),ncol(dta),byrow=FALSE))))
      res <- svd(matrix(ystar, nrow(dta), ncol(dta), byrow = FALSE), nu = 1)
      b <- as.vector(res$v[, 1] * res$d[1])
      
      omega1 <- as.vector(res$u) - res$u[1, 1]
      omega <- omega1/sd(omega1)
      b <- b * sd(omega1)
      
      
      
      # Create holding bins for some stuff for the convergence code
      min1 <- c(rep(1, nrow(dta) - 1))
      min2 <- c(rep(1, ncol(dta)))
      iter <- 0
      conv <- 0
      diffparam <- 0
      
      # Put everything together in a list
      list(alpha = as.vector(alpha), psi = as.vector(psi), b = b, omega = omega,
           min1 = min1, min2 = min2, iter = iter, conv = conv, diffparam = diffparam)
    }
    
  } else {
    
    
    rockingstarts <- function(dta, fixval) {
      cat("Performing fix two omega starting value calc\n")
      P <- nrow(dta)
      W <- ncol(dta)
      numword <- rep(1:W, each = P)
      numparty <- rep(1:P, W)
      dat <- matrix(1, nrow = W * P, ncol = 3)
      dat[, 1] <- as.vector(as.matrix(dta))
      dat[, 2] <- as.vector(numword)
      dat[, 3] <- as.vector(numparty)
      dat <- data.frame(dat)
      colnames(dat) <- c("y", "word", "party")
      dat$word <- factor(dat$word)
      dat$party <- factor(dat$party)
      
      # Starting values for psi
      psi <- log(colMeans(dta))
      # Starting values for alpha
      alpha <- log(rowMeans(dta)/rowMeans(dta)[1])
      
      # Starting values for beta and x
      ystar <- log(dat$y + 0.1) - alpha[dat$party] - psi[dat$word]
      res <- svd(matrix(ystar, nrow(dta), ncol(dta), byrow = FALSE), nu = 1)
      b <- as.vector(res$v[, 1] * res$d[1])
      
      omega <- as.vector(res$u)
      
      
      
      
      # Create holding bins for some stuff for the convergence code
      min1 <- c(rep(1, nrow(dta) - 1))
      min2 <- c(rep(1, ncol(dta)))
      iter <- 0
      conv <- 0
      diffparam <- 0
      
      # Put everything together in a list
      list(alpha = as.vector(alpha), psi = as.vector(psi), b = b, omega = omega,
           min1 = min1, min2 = min2, iter = iter, conv = conv, diffparam = diffparam)
    }
    
    
  }
  
  
  # Log-Likelihood Functions (Poisson model)
  # ========================================
  
  llik_psi_b <- function(p, y, omega, alpha, sigma) {
    # beta and psi will be estimated
    b <- p[1]
    psi <- p[2]
    lambda <- exp(psi + alpha + b * omega)  # Lambda parameter for Poisson distribution
    -(sum(-lambda + log(lambda) * y) - 0.5 * (b^2/sigma^2))  # Log-likelihood including normal prior on Beta
  }
  
  
  llik_alpha_1 <- function(p, y, b, psi) {
    # omega[1] is estimated
    omega <- p[1]
    lambda <- exp(psi + b * omega)  # Lambda parameter; alpha is excluded b/c it is set to be zero
    -sum(-lambda + log(lambda) * y)  # Log-likelihood
  }
  
  llik_alpha_omega <- function(p, y, b, psi) {
    # all other omegas and alphas are estimated
    omega <- p[1]
    alpha <- p[2]
    lambda <- exp(psi + alpha + b * omega)  # Lambda parameter
    -sum(-lambda + log(lambda) * y)  # Log-likelihood
  }
  
  
  llik_justalpha <- function(p, y, b, psi, omega) {
    # alpha is estimated
    alpha <- p[1]
    lambda <- exp(psi + alpha + b * omega)  # Lambda parameter
    -sum(-lambda + log(lambda) * y)  # Log-likelihood
  }
  
  
  
  
  
  if (fixtwo == FALSE) {
    
    
    cat("Performing mean 0 sd 1 EM algorithm\n")
    # Expectation-Maximization Algorithm FOR MEAN 0, SD 1 IDENTIFICATION
    # ==================================================================
    
    rockingpoisson <- function(dta, tol, sigma, params = NULL, dir = dir, printsum = TRUE) {
      
      P <- nrow(dta)
      W <- ncol(dta)
      
      if (is.null(params)) {
        params <- rockingstarts(dta)  # Call up starting value calculation
      }
      
      iter <- 2
      maxllik <- cbind(-1e+70, rep(0, 1400))
      ll.words <- matrix(-1e+70, W, 1400)
      diffllik <- 500
      
      # Set the convergence criterion
      conv <- tol
      params$conv <- conv
      
      while (diffllik > conv) {
        # Run algorithm if difference in LL > convergence criterion
        omegaprev <- params$omega
        bprev <- words_weight
        alphaprev <- params$alpha
        psiprev <- params$psi
        
        # ESTIMATE OMEGA AND ALPHA
        
        if (printsum == TRUE) {
          cat("Iteration", iter - 1, "\n")
          cat("\tUpdating alpha and omega..\n")
        }
        
        
        # Estimate first omega (alpha is set to 0)
        resa <- optim(p = c(params$omega[1]), fn = llik_alpha_1, y = as.numeric(dta[1,
        ]), b = words_weight, psi = params$psi, method = c("BFGS"))
        params$omega[1] <- resa$par[1]
        params$min1[1] <- -1 * resa$value
        params$alpha[1] <- 0
        ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
               NA)
        
        
        # Estimate all other omegas and alphas
        for (i in 2:P) {
          resa <- optim(par = c(params$omega[i], params$alpha[i]), fn = llik_alpha_omega,
                        y = as.numeric(dta[i, ]), b = words_weight, psi = params$psi)
          params$omega[i] <- resa$par[1]
          params$alpha[i] <- resa$par[2]
          params$min1[i] <- -1 * resa$value
          ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                 NA)
          
        }
        
        flush.console()
        
        
        # Z-score transformation of estimates for omega (to identify model)
        omegabar <- mean(params$omega)
        b1 <- words_weight
        params$omega <- (params$omega - omegabar)/sd(params$omega)
        params$psi <- params$psi + b1 * omegabar
        
        # Global identification
        if (params$omega[dir[1]] > params$omega[dir[2]]) {
          params$omega <- params$omega * (-1)
        }
        
        
        
        # ESTIMATE PSI AND BETA
        if (printsum == TRUE) {
          cat("\tUpdating psi and beta..\n")
        }
        
        for (j in 1:W) {
          resb <- optim(par = c(words_weight[j], params$psi[j]), fn = llik_psi_b,
                        y = dta[, j], omega = params$omega, alpha = params$alpha, sigma = sigma)
          params$psi[j] <- resb$par[2]
          params$min2[j] <- -1 * resb$value
          ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                 NA)
        }
        
        flush.console()
        
        # Calculate Log-Likelihood
        maxllik[iter] <- sum(params$min2)
        diffparam <- mean(abs(params$omega - omegaprev))  # difference btw current & previous estimate for omega
        
        ll.words[, iter] <- params$min2
        diff.ll.words <- (ll.words[, iter] - ll.words[, iter - 1])
        diffllik <- sum(diff.ll.words)/abs(maxllik[iter])
        
        
        if (printsum == TRUE) {
          # print(sum(diff.ll.words)) print(abs(maxllik[iter]))
          cat("\tConvergence of LL: ", diffllik, "\n")
        }
        
        params$diffllik[iter - 1] <- diffllik
        params$diffparam[iter - 1] <- diffparam
        params$diffparam.last <- diffparam
        params$maxllik[iter - 1] <- maxllik[iter]
        params$iter <- iter - 1
        iter <- iter + 1
      }
      params$diffllik[1] <- NA
      return(params)
    }
    
    # Run the algorithm
    est <- rockingpoisson(dta, tol, sigma, dir = dir)
  } else {
    cat("Performing fix two omega EM algorithm\n")
    
    # Expectation-Maximization Algorithm FOR TWO FIXED OMEGAS
    # ==================================================================
    
    rockingpoisson <- function(dta, tol, sigma, params = NULL, fixdoc = fixdoc,
                               printsum = TRUE) {
      
      P <- nrow(dta)
      W <- ncol(dta)
      
      if (is.null(params)) {
        params <- rockingstarts(dta, fixval = fixdoc)  # Call up starting value calculation
      }
      
      iter <- 2
      maxllik <- cbind(-1e+70, rep(0, 1000))
      ll.words <- matrix(-1e+70, W, 1000)
      
      diffllik <- 500
      
      # Set the convergence criterion
      conv <- tol
      params$conv <- conv
      
      while (diffllik > conv) {
        # Run algorithm if difference in LL > convergence criterion
        omegaprev <- params$omega
        bprev <- params$b
        alphaprev <- params$alpha
        psiprev <- params$psi
        
        # ESTIMATE OMEGA AND ALPHA
        
        if (printsum == TRUE) {
          cat("Iteration", iter - 1, "\n")
          cat("\tUpdating alpha and omega..\n")
        }
        
        
        # Set omegas and first alpha
        
        params$omega[fixdoc[1]] <- fixdoc[3]
        params$omega[fixdoc[2]] <- fixdoc[4]
        params$alpha[1] <- 0
        
        
        if (1 %in% fixdoc[1:2] == TRUE) {
          
          # if first doc is one of the fixed omegas, do nothing (alpha and omega are fixed)
          
        } else {
          # Estimate first omega (alpha is set to 0)
          resa <- optim(p = c(params$omega[1]), fn = llik_alpha_1, y = as.numeric(dta[1,
          ]), b = params$b, psi = params$psi, method = c("BFGS"))
          params$omega[1] <- resa$par[1]
          params$min1[1] <- -1 * resa$value
          params$alpha[1] <- 0
          ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                 NA)
        }
        
        
        
        
        # Estimate all other omegas and alphas
        for (i in 2:P) {
          
          
          if (sum(fixdoc[1:2] == i) == 1) {
            
            # Estimate just alpha
            resa <- optim(par = params$alpha[i], fn = llik_justalpha, y = as.numeric(dta[i,
            ]), b = params$b, psi = params$psi, omega = params$omega[i],
            method = c("BFGS"))
            params$alpha[P] <- resa$par[1]
            
            ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                   NA)
            
          } else {
            resa <- optim(par = c(params$omega[i], params$alpha[i]), fn = llik_alpha_omega,
                          y = as.numeric(dta[i, ]), b = params$b, psi = params$psi)
            params$omega[i] <- resa$par[1]
            params$alpha[i] <- resa$par[2]
            params$min1[i] <- -1 * resa$value
            ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                   NA)
          }
          
        }
        
        
        flush.console()
        
        
        
        # ESTIMATE PSI AND BETA
        if (printsum == TRUE) {
          cat("\tUpdating psi and beta..\n")
        }
        
        for (j in 1:W) {
          resb <- optim(par = c(params$b[j], params$psi[j]), fn = llik_psi_b,
                        y = dta[, j], omega = params$omega, alpha = params$alpha, sigma = sigma)
          params$b[j] <- resb$par[1]
          params$psi[j] <- resb$par[2]
          params$min2[j] <- -1 * resb$value
          ifelse(resa$convergence != 0, print("Warning: Optim Failed to Converge!"),
                 NA)
        }
        
        flush.console()
        
        # Calculate Log-Likelihood
        maxllik[iter] <- sum(params$min2)
        diffparam <- mean(abs(params$omega - omegaprev))  # difference between current and previous estimate for omega
        
        ll.words[, iter] <- params$min2
        diff.ll.words <- (ll.words[, iter] - ll.words[, iter - 1])
        diffllik <- sum(diff.ll.words)/abs(maxllik[iter])
        
        # print(sum(diff.ll.words)) print(abs(maxllik[iter]))
        if (printsum == TRUE) {
          cat("\tConvergence of LL: ", diffllik, "\n")
        }
        
        params$diffllik[iter - 1] <- diffllik
        params$diffparam[iter - 1] <- diffparam
        params$diffparam.last <- diffparam
        params$maxllik[iter - 1] <- maxllik[iter]
        params$iter <- iter - 1
        iter <- iter + 1
      }
      params$diffllik[1] <- NA
      return(params)
    }
    
    # Run the algorithm
    est <- rockingpoisson(dta, tol, sigma, fixdoc = fixdoc)
  }
  
  cat("======================================\n")
  cat("WORDFISH 2 DLA ML Estimation finished.\n")
  cat("======================================\n\n")
  
  # Write output
  output.documents <- cbind(est$omega, est$alpha)
  rownames(output.documents) <- rownames(dta)
  colnames(output.documents) <- c("omega", "alpha")
  
  # Write estimation output file Include: Log-likelihood, iterations, number of
  # words, number of documents
  
  output.estimation <- cbind(nword, nparty, est$iter, sum(est$min2), est$conv,
                             est$diffparam.last)
  colnames(output.estimation) <- c("Words", "Documents", "Iterations", "Log-Likelihood",
                                   "Convergence Criterion", "Difference in X")
  
  if (writeout == TRUE) {
    write.table(output.documents, file = paste(output, "documents.csv", sep = "_"))
    write.table(output.words, file = paste(output, "words.csv", sep = "_"))
    write.table(output.estimation, file = paste(output, "estimation.csv", sep = "_"))
  }
  
  ########################### Parametric Bootstrap Code
  
  bootstrap <- function(nsim, output.documents, output.words, nparty, nword) {
    
    cat("STARTING PARAMETRIC BOOTSTRAP\n")
    
    # input alpha and omega from estimation
    alpha.omega <- output.documents
    
    # input psis and betas from estimation
    psi.beta <- output.words
    
    # Create matrix of results.
    output.se.omega <- matrix(0, nparty, nsim)
    output.se.b <- matrix(0, nword, nsim)
    
    alpha <- alpha.omega[, 2]
    omega <- alpha.omega[, 1]
    psi <- psi.beta[, 2]
    b <- psi.beta[, 1]
    
    # create data matrix
    dtasim <- matrix(1, nrow = nparty, ncol = nword)
    cat("======================================\n")
    cat("Now running", nsim, "bootstrap trials.\n")
    cat("======================================\n")
    cat("Simulation ")
    
    for (k in 1:nsim) {
      
      cat(k, "...")
      
      # Generate new data using lambda
      for (i in 1:nparty) {
        dtasim[i, ] <- rpois(nword, exp(psi + alpha[i] + b * omega[i]))
      }
      
      alphastart <- alpha + rnorm(length(alpha.omega[, 1]), mean = 0, sd = (sd(alpha.omega[,
                                                                                           2])/2))
      omegastart <- omega + rnorm(length(alpha.omega[, 1]), mean = 0, sd = (sd(alpha.omega[,
                                                                                           1])/2))
      psistart <- psi + rnorm(length(psi.beta[, 1]), mean = 0, sd = (sd(psi.beta[,
                                                                                 2])/2))
      bstart <- b + rnorm(length(psi.beta[, 1]), mean = 0, sd = (sd(psi.beta[,
                                                                             1])/2))
      params <- list(alpha = alphastart, omega = omegastart, psi = psistart,
                     b = bstart)
      
      
      if (fixtwo == FALSE) {
        est <- rockingpoisson(dtasim, tol, sigma, params = params, dir = dir,
                              printsum = FALSE)
      } else {
        est <- rockingpoisson(dtasim, tol, sigma, params = params, fixdoc = fixdoc,
                              printsum = FALSE)
      }
      
      
      # Store omegas
      output.se.omega[, k] <- est$omega
      # Store Bs
      output.se.b[, k] <- words_weight
    }
    
    
    conf.documents <- matrix(0, nparty, 4)
    colnames(conf.documents) <- c("LB", "UB", "Omega: ML", "Omega: Sim Mean")
    rownames(conf.documents) <- rownames(dta)
    for (i in 1:nparty) {
      conf.documents[i, 1] <- quantile(output.se.omega[i, ], 0.025)
      conf.documents[i, 2] <- quantile(output.se.omega[i, ], 0.975)
      conf.documents[i, 3] <- omega[i]
      conf.documents[i, 4] <- mean(output.se.omega[i, ])
    }
    
    
    
    # CI for word weights
    conf.words <- matrix(0, nword, 4)
    colnames(conf.words) <- c("LB", "UB", "B: ML", "B: Sim Mean")
    rownames(conf.words) <- words
    
    
    for (i in 1:nword) {
      conf.words[i, 1] <- quantile(output.se.b[i, ], 0.025)
      conf.words[i, 2] <- quantile(output.se.b[i, ], 0.975)
      conf.words[i, 3] <- b[i]
      conf.words[i, 4] <- mean(output.se.b[i, ])
    }
    
    return(list(conf.documents = conf.documents, conf.words = conf.words))
  }
  
  if (boots == TRUE) {
    bootresult <- bootstrap(nsim, output.documents, output.words, nparty, nword)
    ci.documents <- bootresult$conf.documents
    ci.words <- bootresult$conf.words
    
    if (writeout == TRUE) {
      write.table(ci.words, file = paste(output, "words_95_ci.csv", sep = "_"))
      write.table(ci.documents, file = paste(output, "documents_95_ci.csv",
                                             sep = "_"))
    }
    
  }
  
  if (boots == F) {
    ci.documents <- NULL
    ci.words <- NULL
  }
  
  
  cat("Finished!\n")
  
  return(list(documents = output.documents, diffllik = est$diffllik, diffomega = est$diffparam,
              maxllik = est$maxllik, estimation = output.estimation, ci.documents = ci.documents,
              ci.words = ci.words))
  
}


```

